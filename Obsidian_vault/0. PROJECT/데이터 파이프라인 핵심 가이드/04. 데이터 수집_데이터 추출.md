# ELT 패턴

- **ELT(Extract, Load, Transform)** 패턴은 데이터 분석, 데이터 사이언스, 데이터 제품 구축에 적합한 데이터 파이프라인 설계 방식

	 ## ETL 패턴
	 - 데이터가 로드되기 전에 변환 과정을 거침
	 - 전통적으로 많이 사용되어 온 방식
	 - 변환 과정에서 데이터의 일관성, 품질을 높일 수 있음
	 - 대규모 데이터 처리에는 시간이 더 걸릴 수도 있음
	   
	 ## ELT 패턴
	 - 데이터를 로드한 후 변환 작업을 수행하기 때문에 대용량 데이터 처리에 유리함
	 - 최신 데이터베이스나 데이터 웨어하우스(ex: Snowflake, Amazon Redshift, Google BigQuery)에서는 이 방식이 더 많이 사용됨
	 - 원본 데이터를 유지하면서 필요한 시점에만 변환 작업을 수행할 수 있어 유연성이 높음
	 - 초기 로드 속도가 빠르며, 데이터 변환 작업을 나중에 수행할 수 있음
	   
	   ### ETL: 데이터 변환 먼저 수행 후, 목적지에 로드
	   ### ELT: 데이터 먼저 로드 후, 이후에 변환 작업을 수행

---
# 파이썬 환경 설정

  - `virtualenv` 해당 장에 사용된 라이브러리를 설치하기 전에 가상환경을 생성하고 활성화한다.
  - 가상환경 생성: `$ python -m venv env`
  - 가상환경 활성화: `$ source env/bin/activate` -> `(env) $` : 명령 프롬프트 앞에 환경 이름이 붙음
  - `(env) $which python` -> `env/bin/python` : `which python` 명령을 사용하여 찾는 위치를 확인할 수도 있다. `which python` 명령어를 사용하면 가상 환경 디렉토리의 경로를 보여준다.
	  - Note: 일부 운영체제에서는 Python 3.x 파일을 실행하려면 python 대신 python3를 사용해야 한다. 이전 OS 버전은 기본적으로 Python 2.x일 수 있다. `python --version`을 입력하여 OS에서 사용하는 파이썬 버전을 확인해야 한다.

---
# 필수 라이브러리 설치

- `configparser` 라이브러리 설치: `(env) $ pip install configparser`
	- configparser 라이브러리는 파일에 추가할 구성 정보를 읽는 데 사용된다.
- `pipeline.conf ` 파일: 파이썬 스크립트를 생성할 디렉토리에 pipeline.conf 비워진 파일을 만든다.
	- `(env) $touch pipeline.conf`: 비워진 .conf 파일 만들기
- `boto3` 라이브러리 설치: `(env) $ pip install boto3`
	- 여기서 boto3는 Python용 AWS SDK를 의미함
---
# S3(클라우드 파일 스토리지 설정)

**Amazon S3(Simple Storage Service)**
- 객체 스토리지 서비스: 파일만 가능
    - 파일 설치 불가(이미지,동영상,워드등 업로드, 삭제, 업데이트만 가능하다)
- 무제한 용량
    - 단 하나의 객체는 0byte에서 5TB용량
    - 하나의 파일은 용량제한이 있지만 그 파일을 몇개 업로드하는지는 무제한
- Bucket이라는 단위로구분
    - '디렉토리'의 개념
    - Bucket 이름은 Global Unique, 즉 세계 어디에서도 중복된 이름이 존재할 수 없음(아래 링크 영상 참조)
    - https://www.youtube.com/watch?v=propgtDEMgM&t=259s
    - Web Hosting시 도메인과 Bucket 명이 같아야함
- 99.999999999% 내구성
    - 파일을 잃어버릴 확률이 0.000000001 %에 수렴한다는 의미!

---
# S3 버킷 설정

- 파일 저장에 AWS S3 버킷을 사용한다.
- S3 버킷을 생성하고, 적절한 액세스 권한을 설정해야 한다.
- **IAM 사용자 생성:**
	1. AWS 콘솔(또는 상단 탐색 모음)의 서비스 메뉴에서 IAM으로 이동
	2. 탐색 창에서 사용자를 클릭한 뒤 [사용자 추가]를 누르고, 사용자 이름을 입력한다. 예시에서는 사용자 이름을 'data_pipeline_readwrite'로 지정함.
	3. 해당 IAM 사용자의 액세스 유형을 클릭한다. 이 사용자는 AWS 콘솔에 로그인할 필요가 없고, 파이썬 스크립트를 통해 프로그래밍 방식으로 AWS 리소스에 액세스할 수 있으므로 [프로그램 방식 액세스]를 클릭한다.
	4. [다음: 권한] 버튼 클릭
	5. [권한 설정] 페이지에서 [기존 정책을 직접 연결] 옵션 클릭. [AmazonS3FullAccess] 정책 선택.
	6. [다음: 태그] 버튼을 클릭한다. 나중에 찾을 수 있도록 다양한 개체와 서비스에 태그를 추가하는 것이 AWS의 모범 사례이다(선택 사항).
	7. [다음: 검토] 버튼을 클릭하여 설정사항들을 확인한다. 모든 것이 정상인 경우, [사용자 만들기]를 클릭한다.
	8. 새로운 IAM 사용자의 액세스 키 ID 및 비밀 액세스 키를 저장할 수 있다. [.csv 다운로드]를 클릭한 다음 파일을 안전한 위치에 저장하여 잠시 후에 사용 가능하다.

  ---
# 구성 파일 설정

- `pipeline.conf` 파일을 생성하여, IAM 사용자 및 S3 버킷 정보를 저장할 섹션을 추가한다.

- **pipeline.conf**
  ```
  [aws_boto_credentials]
  access_key = your_access_key
  secret_key = your_secret_key
  bucket_name = your_bucket_name
  account_id = your_account_id
  ```

---
### p.65 - 70

# REST API에서 데이터 추출

- REST API는 데이터를 추출하는 흔한 방법 중 하나
- 조직에서 직접 만들고 유지 관리하는 API에서 데이터를 수집하거나, 외부 서비스/공급업체의 API에서 데이터를 수집한다.

## 일반적인 데이터 추출 패턴:

1. API 엔드포인트로 HTTP GET 요청 전송
2. (주로)JSON 형식의 응답 수신
3. 응답 파싱 후 CSV 파일로 변환 (데이터 웨어하우스 로드용)

> 참고: JSON 형식 그대로 저장할 수도 있지만, 이 예제에서는 CSV 사용

---
## OpenNotify API 예제

이 예제에서는 국제 우주 정거장(ISS)의 위치 정보를 제공하는 OpenNotify API를 사용함(링크: http://open-notify.org/Open-Notify-API/)

### API 쿼리 준비

먼저 필요한 라이브러리를 임포트한다.

```python
# REST API 요청을 위한 라이브러리
import requests
# JSON 데이터 처리를 위한 라이브러리
import json
# 설정 파일 읽기를 위한 라이브러리
import configparser
# CSV 파일 작성을 위한 라이브러리
import csv
# AWS S3 연동을 위한 라이브러리
import boto3
```

### API 쿼리 실행

```python
# ISS 위치 조회를 위한 위도와 경도 설정
lat = 42.36
lon = 71.05
# API 요청 파라미터 설정
lat_log_params = {"lat": lat, "lon": lon}

# OpenNotify API에 GET 요청 보내기
api_response = requests.get(
    "http://api.open-notify.org/iss-pass.json", params=lat_log_params
)
```

### 응답 처리 및 CSV 파일 생성

```python
# API 응답을 JSON 객체로 변환
response_json = json.loads(api_response.content)
# 모든 통과 정보를 저장할 리스트 초기화
all_passes = []

# 각 통과 정보에 대해 반복
for response in response_json['response']:
    current_pass = []
    # 위도와 경도 추가
    current_pass.append(lat)
    current_pass.append(lon)
    # 지속 시간과 상승 시간 추가
    current_pass.append(response['duration'])
    current_pass.append(response['risetime'])
    # 현재 통과 정보를 전체 리스트에 추가
    all_passes.append(current_pass)

# CSV 파일명 설정
export_file = "export_file.csv"
# CSV 파일 작성
with open(export_file, 'w') as fp:
    csvw = csv.writer(fp, delimiter='|')
    csvw.writerows(all_passes)
```

이 코드는 API 응답을 파싱하여 위도, 경도, 지속 시간, 상승 시간을 CSV 파일로 저장합니다.

### S3 버킷에 파일 업로드

```python
# 설정 파일 parser 생성
parser = configparser.ConfigParser()
# 'pipeline.conf' 파일에서 설정 읽기
parser.read("pipeline.conf")
# AWS 접근 키, 비밀 키, 버킷 이름 가져오기
access_key = parser.get("aws_boto_credentials", "access_key")
secret_key = parser.get("aws_boto_credentials", "secret_key")
bucket_name = parser.get("aws_boto_credentials", "bucket_name")

# AWS S3 클라이언트 생성
s3 = boto3.client('s3',
    aws_access_key_id=access_key,
    aws_secret_access_key=secret_key)

# CSV 파일을 S3 버킷에 업로드
s3.upload_file(
    export_file, bucket_name, export_file
)
```

이 코드는 설정 파일에서 AWS 자격 증명을 읽어와 Boto3 라이브러리를 사용해 CSV 파일을 S3 버킷에 업로드한다.

---
## 참고사항

- 특정 API용 Python 라이브러리가 있다면 변경하여 실습 가능 (예: Twitter API용 tweepy)
- 데이터 웨어하우스 로딩에 대한 자세한 내용은 Chapter 5 참조
- CSV 외 형식으로 데이터 로드하는 방법은 데이터 웨어하우스 설명서 참조