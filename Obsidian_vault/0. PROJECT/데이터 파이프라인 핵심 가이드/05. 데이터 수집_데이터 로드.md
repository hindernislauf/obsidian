## Snowflake 데이터 웨어하우스를 S3 버킷과 연동하는 구성 방법

- Snowflake 인스턴스에서 S3 버킷에 대한 액세스를 구성하는 세 가지 옵션
	1. **Snowflake 스토리지 통합 구성** (권장)
	2. **AWS IAM 역할 구성**
	3. **AWS IAM 사용자 구성**

- 첫 번째 방법인 **Snowflake 스토리지 통합**은 이후 Snowflake에서 S3 버킷과 상호작용할 때 더욱 원활하게 동작하기 때문에 권장되는 옵션
- 이 구성에는 여러 단계가 포함되므로 최신 Snowflake 설명서를 참조하는 것이 좋음

[참조 링크](https://docs.snowflake.com/en/user-guide/data-load-s3-config.html#option-1-configuring-a-snowflake-storage-integration)

## 외부 단계(External Stage) 구성
- 구성의 마지막 단계에서는 **외부 단계(external stage)** 를 만듦 
- 외부 단계는 Snowflake가 액세스할 수 있도록 외부 저장 위치를 가리키는 객체 
- 이전에 생성한 S3 버킷이 해당 위치로 사용됨

### FILE FORMAT 정의

```sql
CREATE or REPLACE FILE FORMAT pipe_csv_format
TYPE = 'CSV'
FIELD_DELIMITER = '|';
```
- 외부 단계 생성 전 Snowflake에서 FILEFORMAT 정의
	- 유사한 파일 형식에 재사용할 수 있기 때문

### 외부 단계 생성 구문

```sql
USE SCHEMA my_db.my_schema;

CREATE STAGE my_s3_stage
storage_integration = s3_int
url = 's3://pipeline-bucket/*'
file_format = pipe_csv_format;
```
- 
## Snowflake에 데이터 로드
- 이 외부 단계를 사용해 S3 버킷에서 추출된 데이터를 Snowflake로 로드
## pipeline.conf 파일에 추가할 항목
- 마지막으로 Snowflake로 로그인할 자격 증명이 있는 `pipeline.conf` 파일에 섹션을 추가 
- 지정된 사용자는 생성한 외부 단계에 대해 **USAGE** 권한을 가져야 하며, `account_name` 값은 클라우드 공급자와 계정이 위치한 지역에 따라 형식이 지정
	- 예를 들어, 계정 이름이 `Snowflake_acct1`이고 AWS의 미국 동부(오하이오) 리전에서 호스팅되는 경우 `account_name` 값은 `snowflake_acct1.us-east-2.aws`가 됨 
	- 이 값은 Python에서 `snowflake-connector-python` 라이브러리를 사용하여 Snowflake에 연결하는 데 사용됨

### pipeline.conf에 추가할 섹션

```ini
[snowflake_creds]
username = snowflake_user
password = snowflake_password
account_name = snowflake_acct1.us-east-2.aws
```

---
# Snowflake 데이터 웨어하우스에 데이터 로드

- Snowflake에 데이터를 로드하는 것은 Redshift에 데이터를 로드하는 패턴과 유사함
- 전체, 증분 또는 CDC 데이터 추출을 처리하는 구체적인 방법에 대해서는 논의하지 않으며, 추출된 파일에서 데이터를 로드하는 구문을 설명
- CDC(Change Data Capture): 데이터베이스의 데이터 변경 사항을 식별하고 추적하는 소프트웨어 프로세스

## Snowflake에 데이터 로드하는 메커니즘
- 데이터 로드는 `COPY INTO` 명령 사용 
	- 하나 또는 여러 파일의 내용을 Snowflake 웨어하우스의 테이블에 로드 

> **Note**: Snowflake는 `Snowpipe`라는 데이터 통합 서비스를 제공한다. 이를 통해 `COPY INTO` 명령을 통한 대량 로드를 예약하는 대신 데이터를 지속적으로 로드할 수 있다.

## COPY INTO 명령 예시

S3 버킷에 저장된 CSV 파일을 Snowflake 테이블에 로드
```sql
COPY INTO destination_table
FROM @my_S3_stage/extract_file.csv;
```

여러 파일을 한 번에 로드하려는 경우, `pattern` 매개변수를 사용할 수 있다.

```sql
COPY INTO destination_table
FROM @my_S3_stage
pattern='.*extract.*.csv';
```

이때 로드할 파일 형식은 외부 단계를 생성할 때 설정되었으므로, `COPY INTO` 명령에서 별도로 명시할 필요가 없다.

## 파이썬 스크립트로 Snowflake에 데이터 로드
파이썬 스크립트를 작성해 Snowflake 인스턴스에 연결, COPY INTO를 사용해 CSV 파일의 내용을 테이블에 로드

```bash
(env) $ pip install snowflake-connector-python
```

```python
import snowflake.connector
import configparser

# pipeline.conf에서 자격 증명 로드
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
username = parser.get("snowflake_creds", "username")
password = parser.get("snowflake_creds", "password")
account_name = parser.get("snowflake_creds", "account_name")

# Snowflake에 연결
snow_conn = snowflake.connector.connect(
    user=username,
    password=password,
    account=account_name
)

# COPY INTO 명령 실행
sql = """
COPY INTO destination_table 
FROM @my_S3_stage 
pattern='.*extract.*.csv';
"""
cur = snow_conn.cursor()
cur.execute(sql)
cur.close()
```

---
## 데이터 레이크 and 데이터 웨어하우스

- 데이터 레이크: S3 버킷 또는 다른 클라우드 스토리지에 저장할 때도 있다. 이를 데이터 레이크라고 하며, 정형 또는 반정형 형태의 데이터를 원본 그대로 저장한다.
- 데이터 웨어하우스: 데이터 레이크는 데이터 저장에는 비용이 더 저렴하지만, 데이터 웨어하우스만큼 쿼리에 최적화되어 있지는 않다. 
	- 그래서 사용하는 것이 데이터 웨어하우스다. 하지만 최근 몇 년간 SQL을 사용하여 데이터 레이크의 데이터를 더 쉽게 쿼리할 수 있도록 도와주는 도구들이 등장했다.
### 예시 도구
- **Amazon Athena**: SQL을 사용하여 S3에 저장된 데이터를 쿼리할 수 있는 AWS 서비스
- **Amazon Redshift Spectrum**: Redshift가 S3의 데이터를 외부 테이블로 참조하여 쿼리할 수 있도록 하는 서비스

## 데이터 레이크를 사용하는 경우

- 데이터 웨어하우스 대신에 데이터 레이크를 사용하는 것이 합리적인 경우
	1. **저장 비용 절감**: 대용량 데이터를 데이터 웨어하우스 대신 클라우드 스토리지 기반 데이터 레이크에 저장하는 것이 더 저렴하다.
	2. **비정형 또는 반정형 데이터 처리**: 스키마가 없는 데이터는 레이크에 저장하여 유연하게 처리할 수 있다.
	3. **탐색 단계**: 데이터 사이언티스트나 머신러닝 엔지니어가 탐색 단계에서 데이터를 분석할 때, 데이터 레이크에 저장된 원본 데이터가 유용한 경우가 있다.

---
# 오픈 소스 프레임워크

각 데이터 수집(추출 및 로드 단계 모두)에는 반복적인 단계가 존재하며, 이를 해결하기 위해 다양한 오픈 소스 프레임워크가 등장했다.

## Singer
- 파이썬으로 작성된 오픈 소스 프레임워크 
- 탭(tap)을 사용해 데이터 소스에서 데이터를 추출하고 JSON 형식으로 스트리밍
	- 예를 들어, MySQL 데이터베이스에서 데이터를 추출하여 Google BigQuery 데이터웨어하우스에 로드하려면 MySQL 탭과 BigQuery 대상을 사용할 수 있다.
- Singer를 사용하면 데이터 수집을 예약하고 조정하기 위해 별도의 오케스트레이션 프레임워크를 사용해야 함 
- 오픈 소스 프로젝트이기 때문에 다양한 탭과 대상이 존재하며, 프로젝트에 직접 기여 또한 할 수 있음 
- 또한 Singer는 Docs 문서화가 잘 되어 있으며, 활발한 Slack 및 GitHub 커뮤니티가 있음

- [Singer 홈페이지](https://www.singer.io/)
- [Singer Slack](https://singer-slackin.herokuapp.com/)
- [GitHub 커뮤니티](https://github.com/singer-io)

### 인기 있는 Singer 탭과 대상
| **Taps**            | **Targets**            |
| ------------------- | ---------------------- |
| Google Analytics    | CSV                    |
| Jira                | Google BigQuery        |
| MySQL               | PostgreSQL             |
| PostgreSQL          | Amazon Redshift        |
| Salesforce          | Snowflake              |

---

# 상업적 대안

오픈 소스 외에도 데이터 수집을 쉽게 할 수 있는 상용 제품이 여러 가지 있다.(기본적으로 일정 조정 및 작업 오케스트레이션 기능이 내장되어 있음)

## Stitch & Fivetran

- 두 도구 모두 웹 기반으로 데이터 엔지니어뿐만 아니라 데이터 팀의 다른 전문가들도 접근할 수 있다. 
- 공통 데이터 소스(예: Salesforce, HubSpot, Google Analytics, GitHub 등)에 대한 수백 개의 사전 구축된 커넥터를 제공하며, MySQL, PostgreSQL 등의 데이터베이스와 Amazon Redshift, Snowflake 등의 데이터웨어하우스를 지원

- [Stitch 홈페이지](https://www.stitchdata.com/)
- [Fivetran 홈페이지](https://liveltran.com/)

### 장단점

#### 장점
- **간편한 데이터 수집**: 사전 구축된 커넥터를 통해 별도의 코드 작성 없이 데이터 수집 가능
- **작업 오케스트레이션**: 데이터 수집 일정을 쉽게 예약하고 관리할 수 있음
- **손상 경고**: 수집 중 손상이나 문제가 발생했을 때 경고 설정 가능

#### 단점
- **비용**: 볼륨 기반 가격 모델을 따르며, 수집하는 데이터 양에 따라 비용이 증가
- **공급업체 종속**: 특정 공급업체에 의존할 경우 나중에 다른 도구로 마이그레이션할 때 많은 작업이 필요
- **커스터마이징이 필요한 코딩**: 커넥터가 없는 소스 시스템에 대해 사용자 지정 코드를 작성해야 함

---