# MySQL 데이터베이스에서 데이터 추출

### 전체 또는 증분 MySQL 테이블 추출


### MySQL 데이터의 이진 로그 복제

- **이진 로그(Binary Log)**
	- 데이터베이스 서버에서 수행되는 모든 데이터 변경 작업(`INSERT`, `UPDATE`, `DELETE` 등의 쿼리)을 기록하는 로그 파일
	- 이진 로그는 데이터 복구(recovery), 데이터 복제(replication), 감사(auditing) 목적으로 매우 중요
	- CDC(변경 데이터 캡처)의 한 형태, 대부분의 원본 데이터 저장소에는 사용할 수 있는 CDC 형식이 존재
		- CDC(Change Data Capture) : 데이터베이스에서 발생하는 데이터 변경 사항을 추적하고 캡처하는 기술, 이를 통해 데이터베이스의 상태 변화(데이터의 삽입, 업데이트, 삭제 등)를 감지하고 이 변화를 다른 시스템으로 전파하거나 분석에 활용 가능 
		- CDC는 데이터 통합, 복제, 분석, 백업 등 다양한 용도로 사용
- 대용량 데이터 수집이 필요한 경우 변경사항을 복제하기 위해 MySQL 이진 로그 내용을 사용하는 것이 효과적
	- MySQL 이진 로그는 데이터베이스에서 수행된 모든 작업에 대한 기록을 보관
	- 구성된 방식에 따라 모든 테이블의 생성 또는 수정사항을 포함하여 모든 INSERT, UPDATE, DELETE 작업도 기록

---
##### TIP 사전 구축된 프레임워크 사용 고려
- 이진 로그 복제의 복잡성으로 인해 데이터를 수집하려면 오픈소스 프레임워크 또는 상용 제품을 고려하는 것이 좋음
---

- 데이터 웨어하우스가 MySQL 데이터베이스가 아닌 경우, 단순하게 내장된 MySQL 복제 기능을 사용할 수는 없음
- MySQL이 아닌 곳으로 데이터를 수집하기 위해 이진 로그를 사용하려면 다음과 같은 단계를 수행
	1. *MySQL 서버에서 이진로그를 활성화하고 구성한다.*
		- 이진 로그를 활성화하여 변경사항 추적 : `my.cnf` 파일에서 `log_bin` 설정을 활성화하고, 필요한 다른 옵션(예: `server_id`, `binlog_format`)을 구성
	2. *초기 전체 테이블 추출을 실행하고 로드한다.*
		- 이진 로그를 사용해 실시간 데이터를 동기화하기 전에, 데이터 웨어하우스의 초기 상태를 설정
		- 이 작업을 위해 MySQL의 모든 테이블을 데이터 웨어하우스로 내보내야 함
	3. *지속적으로 이진 로그를 추출한다.*
		- 초기 데이터를 로드한 후에는 MySQL 서버에서 이진 로그를 지속적으로 추출해야 함
		- 이진 로그에 기록된 데이터 변경사항을 실시간으로 데이터 웨어하우스로 전송
	4. *추출된 이진 로그를 데이터 웨어하우스로 변환하여 로드한다.*
		- 추출된 이진 로그는 MySQL 형식으로 저장되므로, 이를 데이터 웨어하우스에서 이해할 수 있는 형식으로 변환 필요

- 이진 로그를 수집하기 전에, 먼저 MySQL 데이터베이스의 현재 상태를 데이터 웨어하우스의 테이블에 반영해야 함
- 그 다음 이진 로그를 사용하여 후속 변경 사항을 수집
- 이를 위해 이진 로그 수집을 켜기 전에 추출한 소스 테이블에 LOCK을 설정한 뒤 해당 테이블에 대해 mysqldump를 실행한 다음 mysqldump 결과를 데이터 웨어하우스의 테이블에 로드해야 함
	- mysqldump : MySQL 및 MariaDB 데이터베이스에서 데이터를 백업하기 위해 사용하는 유틸리티로, 데이터베이스의 구조(테이블 스키마)와 데이터를 SQL 형식의 덤프 파일로 내보낼 수 있음

---
##### TIP 소스 시스템 소유자와의 논의
- MySQL 소스 시스템에서 이진 로그의 구성을 수정하기 위한 접근 권한은 종종 시스템 관리자에게만 부여됨
- 데이터를 수집하려는 데이터 엔지니어는 변경사항이 MySQL 서버뿐만 아니라 다른 시스템에 영향을 미칠 수 있으므로 이진 로그 구성을 변경하기 전에 항상 데이터베이스 소유자와 논의하고 함께 작업해야 함
---

[이진 로그 구성과 관련한 MySQL 데이터베이스 선행 작업]
1. *이진 로깅이 활성화 되어 있는지 확인*
	- 일반적으로 디폴트 값으로 활성화 됨
	- 데이터베이스에서 다음의 SQL 쿼리를 실행하여 확인 가능
	- (참고)이진 로깅 : 이진 로그를 생성하고 관리하는 과정
```
SELECT variable_value AS bin_log_status
FROM performance_schema.g]obal_variables
WHERE variabale_name='log_bin';
```
 → `log_bin`: MySQL 서버에서 이진 로깅 기능이 활성화되어 있는지 여부를 제어하는 시스템 변수로, 변수 값이 `ON`이면 이진 로깅이 활성화된 상태, `OFF`이면 비활성화된 상태

- 이진 로깅을 사용하도록 설정하면 다음이 표시됨
- 반환된 상태가 OFF인 경우 MySQL 설명서를 참조하여 해당 버전을 활성화
```
+-----------------------------+
| bin_log_status :: |
+-----------------------------+
| ON |
+-----------------------------+
```

2. *이진 로깅 형식이 적절히 설정되어 있는지 확인 (이진 로그 포맷 설정)*
	- 주로 사용되는 포맷 : MySQL 최신 버전에서는 **1) STATEMENT, 2) ROW, 3) MIXED** 형식이 지원됨
	- **STATEMENT**
		- 기능 : 이진 로그에 행을 삽입하거나 수정하는 행동들에 대해 SQL문 자체를 기록
		  → 이진 로그에는 `INSERT`, `UPDATE`, `DELETE`와 같은 SQL 쿼리문이 기록됨
		- 하나의 MySQL DB에서 다른 MySQL DB로 데이터를 복제하는 경우 STATMENT 형식이 유용함
		- 데이터 복제 시 모든 SQL문을 실행하여 DB 상태를 복제할 수 있음
		- 추출된 데이터는 타 플랫폼에서 실행되는 데이터 웨어하우스에 바인딩 되어야 하기 때문에, MySQL DB에서 생성된 SQL문이 데이터 웨어하우스와 호환되지 않을 수 있음
	- **ROW**
		- 기능 : 테이블의 행에 대한 모든 변경사항이 SQL문이 아니라 행 자체의 데이터로 이진 로그 행에 표시됨 (기본 형식)
		  → 변경된 행의 데이터가 이진 로그에 직접 저장됨
		- 데이터의 실제 변경 내용을 기록하므로 복제 및 데이터 일관성 유지가 더 정확함
		- 데이터 변경 작업 시 변경된 행의 데이터를 직접 이진 로그에 저장하기 때문에 로그 파일의 크기가 커질 수 있으며, 행 데이터가 저장되기 때문에 디스크 공간을 더 많이 소모함
	- **MIXED**
		- 기능 : 이진 로그에 STATEMENT 형식 레코드와 ROW 형식 레코드를 모두 기록
		- 추후 ROW 데이터만 걸러낼 수 있지만, 이진 로그를 다른 용도로 사용하지 않는 한 결국 디스크 공간을 추가로 사용하게 되기 때문에 MIXED를 활성화 할 필요는 없음

- 다음 SQL 쿼리를 실행하여 현재 이진 로그 형식을 확인
```
SELECT variable_value AS bin_log_format
FROM performance_schema.g]obal_variables
WHERE variabale_name='binlog_format';
```

- 현재 활성화된 형식을 반환
```
+-----------------------------+
| bin_log_status :: |
+-----------------------------+
| ON |
+-----------------------------+
```

- 이진 로그 형식과 기타 설정은 MySQL DB 인스턴스 관련 my.cnf 파일에서 설정됨
- 파일을 열면 다음과 같은 행이 표시됨
```
[mysqld]
binlog_format=row
```

- 이진 로깅이 ROW 형식으로 사용되도록 설정되었으므로 관련 정보를 추출하여 데이터 웨어하우스에 로드할 파일에 저장하는 프로세스를 구축할 수 있음
- ROW 형식 이벤트 세 가지 유형
	- **WRITE_ROWS_EVENT**
	- **UPDATE_ROWS_EVENT**
	- **DELETE_ROWS_EVENT**

[이진 로그에서 이벤트 가져오기]
- (참고)이진 로그에서 이벤트를 가져오는 과정과 관련된 코드는 다음과 같은 기능을 수행
	1. MySQL 서버에 연결하여 이진 로그를 읽는 설정을 구성
	2. BinLogStreamReader 클래스를 사용하여 이진 로그를 스트리밍하고, **DELETE_ROWS_EVENT**, **WRITE_ROWS_EVENT**, **UPDATE_ROWS_EVENT** 이벤트를 읽음
	3. 이벤트 출력 및 스트리밍 종료 후 리소스를 정리

- 오픈소스 파이썬 라이브러리(python-mysql-replication) 활용
```
(env) $pip install mysql-replication
```

- 이진 로그 출력 방식을 알아보기 위해 DB에 연결하고 이진 로그 읽어보기
	[예제]
	- 앞부분에 나왔던 예제에 대해 pipeline.conf 파일에 추가된 MySQL 연결 정보를 사용
	- MySQL 서버의 기본 이진 로그 파일에서 읽음
	- 기본 이진 로그 파일 이름 및 경로는 MySQL DB의 `my.cnf` 파일에 저장된 `log_bin` 변수에 설정됨
	- 이진 로그가 시간(매일 또는 매시간)에 따라 순환되는 경우,
		- MySQL 관리자가 선택한 로그 순환 방법 및 파일 이름 지정 체계에 따라 파일 경로를 결정하고
		- BinLogStreamReader 인스턴스를 만들 때 해당 경로를 log_file 매개변수에 값으로 전달해야 함
```
from pymysqlreplication import BinLogStreamReader
from pymysqlreplication import row_event
import configparser
import pymysqlreplication

# MySQL 연결 정보 가져옴
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
hostname = parser.get("mysql_config", "hostname")
port = parser.get("mysql_config", "port")
username = parser.get("mysql_config", "username")
password = parser.get("mysql_config", "password")

mysql_settings = {
	"host": hostname,
	"port": int(port),
	"user": username,
	"passwd":password
}

b_stream = BinLogStreamReader(
			connection_settings = mysql_settings,
			server_id=100,
			only_events=[row_event.DeleteRowsEvent,
						row_event.WriteRowsEvent,
						row_event.UpdateRowsEvent]
			)
for event in b_stream:
	evnet.dump()

b_stream.close()
```

- 인스턴스화된 BinLogStreamReader 개체는 pipeline.conf 파일에 지정된 MySQL DB에 연결하고 특정 이진 로그 파일을 읽음 
- `resume_stream=True` 설정과 `log_pos` 값은 지정된 지점에서 이진 로그를 읽기 시작함
	→ 예제에서는 1400번 위치
	→ `resume_stream=True`는 `mysql-replication` 라이브러리에서 BinLogStreamReader를 사용하여 이진 로그를 읽을 때, 이전에 중단된 위치에서 계속해서 로그를 읽도록 설정하는 옵션
- BinLogStreamReader에게 DeleteRowsEvent, WriteRowsEvent, UpdateRowsEvent만 읽도록 지시
- 스크립트는 모든 이벤트를 반복하고 사람이 읽을 수 있는 형식으로 출력
- Orders 테이블이 있는 DB의 경우 다음과 같은 항목이 출력됨
```
=== WriteRowsEvnet ===
Date: 2020-06-01 12:00:00
Log position: 1400
Event size: 30
Read bytes: 20
Talbe: orders
Affected columns: 3
Changed rows: 1
Values:
--
* OrderId : 1
* OrderStatus : Backordered
* LastUpdated : 2020-06-01 12:00:00

=== UpdateRowsEvnet ===
Date: 2020-06-09 12:00:25
Log position: 1401
Event size: 56
Read bytes: 15
Talbe: orders
Affected columns: 3
Changed rows: 1
Affected columns: 3
Values:
--
* OrderId : 1 => 1
* OrderStatus : Backordered => Shipped
* LastUpdated : 2020-06-01 12:00:00 => 2020-06-09 12:00:25
```
  → OrderId 1의 INSERT 및 UPDATE를 나타내는 두 가지 이벤트가 있음(표 4-3 참조)

- 예제에서 두 순차적 이진 로그 이벤트는 며칠 간격으로 발생하지만, 실제 환경에서는 두 이벤트 사이에 DB 변경사항을 모두 나타내는 수많은 이벤트가 존재
- log_pos 값 : BinLogStreamReader의 시작 위치를 알려주며, 다음 추출이 실행될 때 선택할 위치를 추적하기 위해 테이블에 저장해야 하는 값
	- 추출이 시작될 때 읽고 쓸 수 있는 데이터 웨어하우스의 로그 테이블에 값을 저장하고 종료 시 최종 이벤트의 위치 값을 저장하는 것이 가장 좋음

| Orderld | OrderStatus | LastUpdated         |
| ------- | ----------- | ------------------- |
| 1       | Backordered | 2020-06-01 12:00:00 |
| 1       | Shipped     | 2020-06-09 12:00:25 |
[표4-3] 데이터 웨어하우스에 있는 모든 버전의 OrderId 1

- 예제 코드를 통해 이벤트를 사람이 읽을 수 있는 형식으로 표시하지만, 출력을 데이터 웨어하우스로 쉽게 로드하려면 다음의 작업 수행 필요
	1. *데이터를 다른 형식으로 구문 분석(parsing)하고 기록한다.*
		- 로딩을 단순화하기 위해 다음 예제에서는 CSV 파일의 행에 각 이벤트를 기록)
		- 이진 로그를 데이터 웨어하우스로 로드하려면, 이벤트를 적절한 형식(CSV)으로 구문 분석하여 기록해야 함
	2. *추출 및 로드하려는 테이블 당 하나의 파일을 작성한다.*
		  - 예제의 이진 로그에는 Orders 테이블과 관련된 이벤트만 포함되지만 실제 이진 로그에는 다른 테이블과 관련된 이벤트도 포함될 가능성이 높음
		  - 각 테이블에 대한 별도의 파일을 생성하거나, 예제처럼 단순성을 위해 하나의 파일에 모든 관련 이벤트를 기록할 수 있음
	- 1번 사항 해결을 위해 .dump() 함수를 사용하는 대신 이벤트 속성을 구문 분석하여 CSV 파일에 씀
	- 2번의 경우 각 테이블에 대한 파일을 작성하는 대신 단순성을 위해 orders_extract.csv 파일에 Orders 테이블과 관련된 이벤트만 기록

- 구현된 전체 추출 프로세스에서 예제 코드를 수정하여 테이블별 이벤트를 그룹화하고, 각 테이블의 변경내용을 별도의 파일로 저장
- 마지막 단계에서는 CSV 파일을 S3 버킷에 업로드하여 데이터 웨어하우스에 로드 가능
```
from pymysqlreplication import BinLogStreamReader
from pymysqlreplication import row_event
import configparser
import pymysqlreplication
import csv
import boto3

# MySQL 연결 정보 가져옴
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
hostname = parser.get("mysql_config", "hostname")
port = parser.get("mysql_config", "port")
username = parser.get("mysql_config", "username")
password = parser.get("mysql_config", "password")

mysql_settings = {
	"host": hostname,
	"port": int(port),
	"user": username,
	"passwd":password
}

b_stream = BinLogStreamReader(
			connection_settings = mysql_settings,
			server_id=100,
			only_events=[row_event.DeleteRowsEvent,
						row_event.WriteRowsEvent,
						row_event.UpdateRowsEvent]
			)

order_events = []

for binlogevent in b_stream:
	for row in binlogevent.rows:
		if binlogevent.table == 'orders':
		event = {}
		if isinstance(
				binlogevent,row_event.DeleteRowsEvent
			):
			event["action"] = "delete"
			event.update(row["values"].items())
		elif isinstance(
			binlogevent,row_event.UpdateRowsEvent
			):
			event["action"] = "update"
			event.update(row["values"].items())
		
		order_events.append(event)

b_stream.close()

keys = order_events[0].keys()
local_filename = 'orders_extract.csv'
with open(
		local_filename,
		'w',
		newline='') as output_file:
	dict_writer = csv.DictWriter(
				output_file, keys, delimiter='|')
	dict_writer.writerows(order_events)

# aws_boto_credentials 값을 로드
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
access_key = parser.get(
				"aws_boto_credentials",
				"access_key")
secret_key = parser.get(
				"aws_boto_credentials",
				"secret_key")

bucket_name = parser.get(
				"aws_boto_credentials", 
				"bucket_name")

s3 = boto3.client(
	's3', 
	aws_access_key_id=access_key, 
	aws_secret_access_key=secret_key)

s3_file = local_filename

s3.upload_file(
	local_filename, 
	bucket_name, 
	s3_file)
```

- 실행 후 orders_extract.csv는 다음과 같이 표시됨
```
insert|1|Backordered|2020-06-01 12:00:00
update|1|Shipped|2020-06-09 12:00:25
```
 
- 결과 CSV 파일의 형식은 빠른 로딩에 최적화되어 있음
  (추출된 데이터 관련은 5~6장에서 진행됨)
