# MySQL 데이터베이스에서 데이터 추출

### 전체 또는 증분 MySQL 테이블 추출

- SQL을 사용한 전체 또는 증분 추출
    - 구현이 간단
    - 자주 변경되는 대규모 데이터세트에서는 확장성이 떨어짐
    - 전체 추출과 증분 추출 사이에도 트레이드오프가 있음(다음 파트)
- 이진 로그(binlog) 복제
    - 구현이 복잡
    - 스트리밍 데이터 수집을 수행하는 경로
    - 원본 테이블의 변경되는 데이터 볼륨이 크거나
    - MySQL 소스에서 데이터를 더 자주 수집해야 하는 경우 적합함

### 설치 방법

1. 로컬 시스템에 설치
2. AWS에서 Amazon RDS를 생성(MySQL 인스턴스 관리)
    1. 샘플을 학습, 작업 → 데이터베이스를 공개적으로 액세스할 수 있게 설정
    2. 프로덕션 설정 → 강력한 보안이 필요하기에 Amazon RDS 보안 모범 사례 따르기 
    3. 단, 비용 발생

```sql
# 테이블 생성
CREATE TABLE Orders ( 
	OrderId int,
	OrderStatus varchar(30), 
	LastUpdated timestamp
);

# 행 삽입
INSERT INTO Orders
	VALUES (1, 'Backordered', '2020-06-01 12:00:00'); 
INSERT INTO Orders
	VALUES (1, 'Shipped', '2020-06-09 12:00:25');
INSERT INTO Orders
	VALUES (2, 'Shipped', '2020-07-11 3:05:00'); 
INSERT INTO Orders
	VALUES (1, 'Shipped', '2020-06-09 11:50:00'); 
INSERT INTO Orders
	VALUES (3, 'Shipped', '2020-07-12 12:00:00');
```

# 전체 또는 증분 MySQL 테이블 추출

전체 추출과 증분 추출은 데이터베이스에서 데이터를 처리하거나 분석할 때 사용하는 두 가지 주요 접근 방식입니다. 각각의 접근 방식은 장단점이 있으며, 선택할 때는 여러 가지 트레이드오프를 고려해야 합니다.

### 전체 추출 (Full Extraction)

```sql
SELECT *
FROM Orders;
```

**장점:**
1. **완전성**: `전체 데이터를 새로 추출`하므로 데이터의 일관성과 정확성을 보장할 수 있습니다. 이전 데이터 상태와 관계없이 항상 최신 상태를 제공합니다. 이전 데이터는 삭제합니다.
2. **단순성**: 데이터베이스 쿼리나 로직이 간단해질 수 있습니다. 모든 데이터를 한 번에 가져오면 별도의 추적이나 비교 작업이 필요 없습니다.

**단점:**
1. **성능**: 대량의 데이터를 매번 새로 추출해야 하므로 데이터베이스와 네트워크에 큰 부하를 줄 수 있습니다. 데이터가 많을수록 성능에 더 큰 영향을 미칩니다.
2. **시간**: 전체 데이터를 매번 추출하고 처리하는 데 시간이 많이 걸릴 수 있습니다.

### 증분 추출 (Incremental Extraction)

```sql
SELECT *
FROM Orders
WHERE LastUpdated > {{ last_extraction_run }};
```

- `LastUpdated`: 데이터베이스 테이블의 데이터가 마지막으로 업데이트된 시점을 나타내는 필드입니다. 이 필드는 일반적으로 `TIMESTAMP` 또는 `DATETIME` 데이터 타입을 가집니다.
- `{{ last_extraction_run }}`: 이 값은 템플릿 문법을 사용하여 외부에서 주입되는 변수입니다. 추출 작업의 최근 실행 시간을 나타내는 타임스탬프입니다. 이 값은 쿼리 실행 시점에 동적으로 설정됩니다.
    
    ```sql
SELECT MAX(LastUpdated)
FROM warehouse.Orders;
    ```
    
- 마지막 데이터 추출 이후에 업데이트된 레코드만을 가져오려면 이 조건을 사용합니다.
- 변경할 수 없는 데이터(레코드를 삽입 가능, 업데이트 불가)가 포함된 테이블의 경우, Last updated 열 대신 레코드가 생성된 시간에 대한 타임스탬프를 사용

**장점:**

1. **성능**: 데이터의 `변화만 추출`하므로, 전체 데이터셋을 새로 추출하는 것보다 효율적입니다. 특히 데이터가 자주 변경되지 않는 경우 유리합니다.
2. **시간**: 데이터 변경 사항만 처리하므로, 전체 추출보다 빠르게 완료될 수 있습니다.

**단점:**

1. **복잡성**: 증분 추출 로직이 더 복잡할 수 있습니다. 변경된 데이터만 추적하고 비교하는 메커니즘이 필요하며, 이를 위한 추가적인 관리가 필요합니다.
2. **일관성 문제**: 변경 사항을 정확하게 추적하지 못하거나 처리 과정에서 데이터 일관성 문제가 발생할 수 있습니다. 예를 들어, 데이터 누락이나 중복이 발생할 수 있습니다.
    1. 삭제된 행은 캡처되지 않는다.
    2. 원본 테이블에는 마지막으로 업데이트된 시간에 대한 신뢰할 수 있는 타임스탬프가 있어야 한다.

### 트레이드오프 요약

- **성능과 시간**: 증분 추출은 성능과 시간 면에서 유리하지만, 전체 추출은 데이터의 일관성과 정확성을 보장합니다.
- **복잡성과 관리**: 증분 추출은 더 복잡한 로직과 관리가 필요하며, 전체 추출은 상대적으로 단순하지만 성능적인 제약이 클 수 있습니다.
- **데이터의 일관성**: 전체 추출은 데이터의 일관성을 유지하는 데 유리하지만, 증분 추출은 일관성을 유지하는 데 더 많은 주의가 필요합니다.

결론적으로, 전체 추출과 증분 추출 중 어떤 방법을 선택할지는 데이터의 크기, 업데이트 빈도, 성능 요구사항, 시스템의 복잡도 등에 따라 달라질 수 있습니다.

### 파이썬 스크립트에 의해 트리거되는 SQL 쿼리를 사용하여 구현

1. 라이브러리 설치

```
(env)  $ pip install pymysql
```

1. 연결 정보를 저장하기 위해 pipeline.conf 파일 업데이트

```python
[mysal_config] 
hostname = my_host.com 
port = 3306
username = my_user_name
password = my_password 
database = db_name
```

1. `extract_mysql full.py`라는 새 파이썬 스크립트를 생성
    1. 수집 로드 단계에서 데이터 웨어하우스로 추출된 데이터를 쉽게 정형화하고 사용 가능

```python
import pymysql 
import csv
import boto3          # S3 버킷에 업로드 위해
import configparser
```

1. MySQL 데이터베이스에 대한 연결을 초기화

```python
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
hostname = parser.get("mysal_config", "hostname")
port = parser.get("mysal_config", "port")
username = parser.get("mysal_config", "username")
dbname = parser.get("mysal_config", "database")
password = parser.get("mysal_config", "password")

conn = pymysql.connect(host=hostname,
	user=username, 
	password=password, 
	db=dbname, 
	port=int(port))
	
if conn is None:
	print("Error connecting to the MySQL database") 
else:
	print("MySQL connection established!")
```

1. Orders 테이블의 전체 추출

```python
m_query = "SELECT * FROM Orders;" 
local_filename = "order_extract.csv"

m_cursor = conn.cursor() 
m_cursor.execute(m_query)
results = m_cursor.fetchall()

with open(local_filename, 'w') as fp: 
	csv_w = csv.writer(fp, delimiter='|') 
	csv_w.writerows(results)
fp.close() 
m_cursor.close() 
conn.close()
```

1. CSV 파일을 S3 버킷에 업로드

```python
# aws_boto_credentials 값을 로드
parser =configparser.ConfigParser() 
parser.read("pipeline.conf")
access_key = parser.get("aws_boto_credentials", "access_key") 
secret_key = parser.get("aws_boto_credentials", "secret_key") 
bucket_name = parser.get("aws_boto_credentials", "bucket_name")

S3 = boto3.client('53', 
aws_access_key_id=access_key, 
aws_secret_access_key=secret_key)

S3_file = local_filename

S3.upload_file(local_filename, bucket_name, S3_file)
```

1. 스크립트 실행

```
(env) $ python extract_mysql_full.py
```

### 데이터 증분 추출을 위한 스크립트 변경사항

1. 스크립트 시작점으로 `extract_mysql_full.py` 파일을 복사하여 `extract_mysql_incremental.py` 사본에서 시작
    ⇒ 소스 데이터베이스에 부담을 주며 프로덕션 쿼리 실행도 차단할 수 있기 때문에 복제본 설정을 고려
    
2. Orders 테이블에서 추출한 마지막 레코드의 타임스탬프를 찾기
    1. Redshift 클러스터와 상호 작용하려면, psycopg2 라이브러리를 설치
    2. Redshift 클러스터에 연결하고 쿼리
    
    ```python
    import psycopg2
    
    # Redshift db connection 정보를 가져옴 
    parser = configparser.ConfigParser() 
    parser.read("pipeline.conf")
    dbname = parser.get("aws_creds", "database")
    user = parser.get("aws_creds", "username") 
    password = parser.get ("aws_creds", "password") 
    host = parser.get("aws_creds", "host")
    port = parser.get ("aws_creds", "port")
    
    # Redshi ft 클러스터에 연결 
    rs_conn = psycopg2. connect(
    	"dbname=" + dbname
    	+ "user=" + user
    	+ "password=" + password 
    	+ "host=" + host
    	+ "port=" + port)
    
    rs_sql = "'SELECT COALESCE(MAXLastUpdated),
    	"1900-01-01')
    	FROM Orders;
    rs_cursor = rs_conn.cursor()
    rs_cursor.execute(rs_sql) 
    result = rs_cursor.fetchone()
    
    # 오직 하나의 레코드만 반환됨
    last_updated_warehouse = result[0]
    
    rs_cursor.close() 
    rs_conn.commit()
    ```
    
    - `last_updated_warehouse` 에 저장된 값을 사용하여 MySQL 데이터베이스에서 실행되는 추출 쿼리를 수정
    - 이전 추출 작업 실행 이후 Orders 테이블에서 업데이트된 레코드만 가져올 수 있음
    - (42p) ~44
    - 새 쿼리에는 last_update_warehouse 값에 대해 %s로 표시되는 자리 표시자가 포함되어 있다. 그러면 값이 튜플(데이터 집합을 저장하는데 사용되는 데이터 유형)로 cursor의 .execute() 함수에 전달된다 . 이는 SQL 주입(injection)을 방지하기 위해 SQL 쿼리에 매개변수를 추가해 주는 적절하고 안전한 방법이다. 다음은 MySQL 데이터베이스에서 SQL 쿼리를 실행해주는 업데이트된 코드다.
    
    ```python
    m_query = """SELECT * 
    	FROM Orders
    	WHERE LastUpdated > %s;""" 
    local_filename = "order_extract.csv"
    
    m_cursor = conn.cursor()
    m_cursor.execute(m_query, (last_updated_warehouse,))
    ```
    
- `extract_mysql_full.py`
    
    ```python
    import pymysql
    import csv
    import boto3
    import configparser
    
    parser = configparser.ConfigParser()
    parser.read("pipeline.conf")
    hostname = parser.get("mysql_config", "hostname")
    port = parser.get("mysql_config", "port")
    username = parser.get("mysql_config", "username")
    dbname = parser.get("mysql_config", "database")
    password = parser.get("mysql_config", "password")
    
    conn = pymysql.connect(host=hostname,
            user=username,
            password=password,
            db=dbname,
            port=int(port))
    
    if conn is None:
      print("Error connecting to the MySQL database")
    else:
      print("MySQL connection established!")
    
    m_query = "SELECT * FROM Orders;"
    local_filename = "order_extract.csv"
    
    m_cursor = conn.cursor()
    m_cursor.execute(m_query)
    results = m_cursor.fetchall()
    
    with open(local_filename, 'w') as fp:
      csv_w = csv.writer(fp, delimiter='|')
      csv_w.writerows(results)
    
    fp.close()
    m_cursor.close()
    conn.close()
    
    # load the aws_boto_credentials values
    parser = configparser.ConfigParser()
    parser.read("pipeline.conf")
    access_key = parser.get("aws_boto_credentials", "access_key")
    secret_key = parser.get("aws_boto_credentials", "secret_key")
    bucket_name = parser.get("aws_boto_credentials", "bucket_name")
    
    s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)
    
    s3_file = local_filename
    
    s3.upload_file(local_filename, bucket_name, s3_file)
    ```
    
- 위 코드는 MySQL 데이터베이스에서 데이터를 추출하여 CSV 파일로 저장하고, 해당 파일을 AWS S3 버킷에 업로드하는 작업을 수행함
- 각 단계에 대한 설명
    1. **MySQL 데이터베이스 연결 정보 로딩**:
        ```python
import configparser

parser = configparser.ConfigParser()
parser.read("pipeline.conf")
hostname = parser.get("mysql_config", "hostname")
port = parser.get("mysql_config", "port")
username = parser.get("mysql_config", "username")
dbname = parser.get("mysql_config", "database")
password = parser.get("mysql_config", "password")
        ```
	- `configparser`를 사용하여 `pipeline.conf` 파일에서 MySQL 데이터베이스 연결 정보를 읽어옴
	- 연결 정보 : 호스트네임, 포트, 사용자 이름, 데이터베이스 이름, 비밀번호를 포함

    2. **MySQL 데이터베이스 연결 및 쿼리 실행**:        
        ```python
import pymysql

conn = pymysql.connect(host=hostname, user=username, password=password, db=dbname, port=int(port))

if conn is None:
  print("Error connecting to the MySQL database")
else:
  print("MySQL connection established!")

  m_query = "SELECT * FROM Orders;"

m_cursor = conn.cursor()
m_cursor.execute(m_query)
results = m_cursor.fetchall()
        ```
	- `pymysql` 라이브러리를 사용하여 MySQL 데이터베이스에 연결
	- 데이터베이스 연결이 성공하면 "MySQL connection established!" 메시지를 출력
	- `"SELECT * FROM Orders;"` 쿼리를 실행하여 `Orders` 테이블의 모든 데이터를 가져옴

    3. **CSV 파일로 데이터 저장**:
        ```python
import csv

local_filename = "order_extract.csv"

with open(local_filename, 'w') as fp:
  csv_w = csv.writer(fp, delimiter='|')
  csv_w.writerows(results)

fp.close()
m_cursor.close()
conn.close()
        ```
	- 가져온 데이터를 `order_extract.csv`라는 파일에 저장
	- CSV 파일을 열고, 데이터를 `|`로 구분된 형식으로 저장

    4. **AWS S3에 파일 업로드**:
        ```python
import boto3

parser = configparser.ConfigParser()
parser.read("pipeline.conf")
access_key = parser.get("aws_boto_credentials", "access_key")
secret_key = parser.get("aws_boto_credentials", "secret_key")
bucket_name = parser.get("aws_boto_credentials", "bucket_name")

s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)

s3_file = local_filename

s3.upload_file(local_filename, bucket_name, s3_file)
        ```
	- `configparser`를 다시 사용하여 AWS S3에 접근하기 위한 인증 정보와 버킷 이름을 읽어옴
	- `boto3` 라이브러리를 사용하여 AWS S3 클라이언트를 생성하고, CSV 파일을 지정된 S3 버킷에 업로드
    
    ### 요약
    1. **MySQL 데이터베이스에서 데이터를 추출**: `Orders` 테이블의 데이터를 쿼리함
    2. **CSV 파일로 저장**: 추출된 데이터를 CSV 파일로 기록
    3. **S3에 업로드**: 생성된 CSV 파일을 AWS S3 버킷에 업로드
    - 위 코드는 데이터 파이프라인의 한 부분으로, 데이터베이스에서 정보를 추출하여 저장하고, 클라우드 스토리지로 백업하는 과정을 자동화함
---
#### Cursor
- 파이썬에서 데이터베이스와 상호작용할 때 `cursor`는 SQL 쿼리를 실행하고 결과를 관리하는 데 중요

    #### cursor의 주요 용도 및 사용법
    1. **SQL 쿼리 실행**:
        - `cursor`는 SQL 명령어를 실행하는 데 사용됩니다. 예를 들어, `SELECT`, `INSERT`, `UPDATE`, `DELETE` 명령어를 실행할 수 있습니다.
        - 예를 들어, 당신의 코드에서:
        여기서 `conn.cursor()`는 새로운 `cursor` 객체를 생성하고, `m_cursor.execute(m_query)`는 `m_query`에 저장된 SQL 쿼리를 실행합니다.
		```python
m_cursor = conn.cursor()
m_cursor.execute(m_query)
		```
    2. **데이터 가져오기**:
        - 쿼리를 실행한 후, `cursor`를 사용하여 결과를 가져올 수 있음
        - `fetchall()`, `fetchone()`, `fetchmany()`와 같은 메서드를 사용하여 쿼리 결과를 가져옴
        - 당신의 코드에서:`fetchall()`은 쿼리의 모든 행을 가져옴
            ```python
results = m_cursor.fetchall()
            ```
    3. **결과 집합 관리**:
        - `cursor`를 사용하여 결과 집합을 한 행씩 탐색 가능
        - 이를 통해 결과를 프로그래밍적으로 처리
    4. **리소스 관리**:
        - `cursor`는 사용 후 닫아야 데이터베이스 리소스를 해제할 수 있음
        - `cursor`를 닫으면 데이터베이스 서버가 할당한 메모리와 리소스가 해제됨
            ```python
m_cursor.close()
            ```
    
    ### 코드에서의 `cursor` 사용 예
    1. **커서 생성**: 데이터베이스 연결 `conn`과 연결된 `cursor` 객체를 생성
        ```python
m_cursor = conn.cursor() 
        ```
    2. **쿼리 실행**: cursor를 사용하여 SQL 쿼리 (`"SELECT * FROM Orders;"`)를 실행
        ```python
m_cursor.execute(m_query)
        ```
    3. **결과 가져오기**: 쿼리 결과의 모든 행을 가져와 `results`에 저장
        ```python
results = m_cursor.fetchall()
        ```
    4. **커서 닫기**: 사용이 끝난 후 `cursor`를 닫아 리소스를 해제
        ```python
m_cursor.close()   
        ```
---


- `extract_mysql_incremental.py`
    ```python
import pymysql
import csv
import boto3
import configparser
import psycopg2

# get db Redshift connection info
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
dbname = parser.get("aws_creds", "database")
user = parser.get("aws_creds", "username")
password = parser.get("aws_creds", "password")
host = parser.get("aws_creds", "host")
port = parser.get("aws_creds", "port")

# connect to the redshift cluster
rs_conn = psycopg2.connect(
	"dbname=" + dbname
	+ " user=" + user
	+ " password=" + password
	+ " host=" + host
	+ " port=" + port)

rs_sql = """SELECT COALESCE(MAX(LastUpdated), '1900-01-01')
	FROM Orders;"""
rs_cursor = rs_conn.cursor()
rs_cursor.execute(rs_sql)
result = rs_cursor.fetchone()

# there's only one row and column returned
last_updated_warehouse = result[0]

rs_cursor.close()
rs_conn.commit()

# get the MySQL connection info and connect
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
hostname = parser.get("mysql_config", "hostname")
port = parser.get("mysql_config", "port")
username = parser.get("mysql_config", "username")
dbname = parser.get("mysql_config", "database")
password = parser.get("mysql_config", "password")

conn = pymysql.connect(host=hostname,
		user=username,
		password=password,
		db=dbname,
		port=int(port))

if conn is None:
  print("Error connecting to the MySQL database")
else:
  print("MySQL connection established!")

m_query = """SELECT *
	FROM Orders
	WHERE LastUpdated > %s;"""
local_filename = "order_extract.csv"

m_cursor = conn.cursor()
m_cursor.execute(m_query, (last_updated_warehouse,))
results = m_cursor.fetchall()

with open(local_filename, 'w') as fp:
  csv_w = csv.writer(fp, delimiter='|')
  csv_w.writerows(results)

fp.close()
m_cursor.close()
conn.close()

# load the aws_boto_credentials values
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
access_key = parser.get(
	"aws_boto_credentials",
	"access_key")
secret_key = parser.get(
	"aws_boto_credentials",
	"secret_key")
bucket_name = parser.get(
	"aws_boto_credentials",
	"bucket_name")

s3 = boto3.client(
	's3',
	aws_access_key_id=access_key,
	aws_secret_access_key=secret_key)

s3_file = local_filename

s3.upload_file(
	local_filename,
	bucket_name,
	s3_file)
    ```
- Amazon Redshift와 MySQL 데이터베이스를 사용하여 데이터를 추출하고, 추출된 데이터를 CSV 파일로 저장한 후, 해당 파일을 AWS S3 버킷에 업로드하는 전체 프로세스를 자동화함
    
1. **Redshift 데이터베이스 연결 정보 로딩**
    ```python
import configparser
import psycopg2

# get db Redshift connection info
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
dbname = parser.get("aws_creds", "database")
user = parser.get("aws_creds", "username")
password = parser.get("aws_creds", "password")
host = parser.get("aws_creds", "host")
port = parser.get("aws_creds", "port")
    ```
    - `configparser`를 사용하여 `pipeline.conf` 파일에서 Amazon Redshift 데이터베이스 연결 정보를 읽어옴
    - 데이터베이스 이름, 사용자 이름, 비밀번호, 호스트, 포트를 변수에 저장
    
 2. **Redshift 클러스터에 연결 및 최신 데이터 추출**    
    ```python
# connect to the redshift cluster
rs_conn = psycopg2.connect(
	"dbname=" + dbname
	+ " user=" + user
	+ " password=" + password
	+ " host=" + host
	+ " port=" + port)

rs_sql = """SELECT COALESCE(MAX(LastUpdated), '1900-01-01')
	FROM Orders;"""
rs_cursor = rs_conn.cursor()
rs_cursor.execute(rs_sql)
result = rs_cursor.fetchone()

# there's only one row and column returned
last_updated_warehouse = result[0]

rs_cursor.close()
rs_conn.commit()
    ```
    - `psycopg2` 라이브러리를 사용하여 Redshift 클러스터에 연결
    - `rs_sql` 쿼리를 실행하여 `Orders` 테이블의 `LastUpdated` 열에서 최대 날짜를 가져옴
       → 데이터의 최신 업데이트 시점을 알 수 있음
    - `fetchone()` 메서드를 사용하여 쿼리 결과를 하나의 행으로 가져오고, `last_updated_warehouse` 변수에 저장
    - 쿼리 실행 후 커서를 닫고 트랜잭션을 커밋
    
3. **MySQL 데이터베이스 연결 정보 로딩 및 연결**
    ```python
import pymysql

# get the MySQL connection info and connect
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
hostname = parser.get("mysql_config", "hostname")
port = parser.get("mysql_config", "port")
username = parser.get("mysql_config", "username")
dbname = parser.get("mysql_config", "database")
password = parser.get("mysql_config", "password")

conn = pymysql.connect(host=hostname,
		user=username,
		password=password,
		db=dbname,
		port=int(port))

if conn is None:
  print("Error connecting to the MySQL database")
else:
  print("MySQL connection established!")
    ```
    - `configparser`를 사용하여 `pipeline.conf` 파일에서 MySQL 데이터베이스 연결 정보를 읽어옴
    - `pymysql` 라이브러리를 사용하여 MySQL 데이터베이스에 연결
    - 연결이 성공하면 "MySQL connection established!"라는 메시지를 출력
4. **MySQL 데이터 쿼리 및 CSV 파일로 저장**
```python
import csv

m_query = """SELECT *
	FROM Orders
	WHERE LastUpdated > %s;"""
local_filename = "order_extract.csv"

m_cursor = conn.cursor()
m_cursor.execute(m_query, (last_updated_warehouse,))
results = m_cursor.fetchall()

with open(local_filename, 'w') as fp:
  csv_w = csv.writer(fp, delimiter='|')
  csv_w.writerows(results)

fp.close()
m_cursor.close()
conn.close()
```
- `m_query` 쿼리를 사용하여 `Orders` 테이블에서 `LastUpdated`가 `last_updated_warehouse`보다 큰 데이터를 선택
- 쿼리 결과를 `order_extract.csv` 파일로 저장
   →  `|`를 구분자로 사용하는 CSV 파일
- 작업이 끝난 후 파일을 닫고 MySQL 커서와 연결을 닫음
    
5. **AWS S3에 CSV 파일 업로드**
    ```python
import boto3

# load the aws_boto_credentials values
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
access_key = parser.get("aws_boto_credentials", "access_key")
secret_key = parser.get("aws_boto_credentials", "secret_key")
bucket_name = parser.get("aws_boto_credentials", "bucket_name")

s3 = boto3.client(
	's3',
	aws_access_key_id=access_key,
	aws_secret_access_key=secret_key)

s3_file = local_filename

s3.upload_file(
	local_filename,
	bucket_name,
        s3_file)
    ```
    - `configparser`를 사용하여 `pipeline.conf` 파일에서 AWS S3에 접근하기 위한 인증 정보와 버킷 이름을 읽어옴
    - `boto3` 라이브러리를 사용하여 AWS S3 클라이언트를 생성
    - `s3.upload_file()` 메서드를 사용하여 `order_extract.csv` 파일을 지정된 S3 버킷에 업로드

---
### MySQL 데이터의 이진 로그 복제

- **이진 로그(Binary Log)**
	- 데이터베이스 서버에서 수행되는 모든 데이터 변경 작업(`INSERT`, `UPDATE`, `DELETE` 등의 쿼리)을 기록하는 로그 파일
	- 이진 로그는 데이터 복구(recovery), 데이터 복제(replication), 감사(auditing) 목적으로 매우 중요
	- CDC(변경 데이터 캡처)의 한 형태, 대부분의 원본 데이터 저장소에는 사용할 수 있는 CDC 형식이 존재
		- **CDC(Change Data Capture)** : 데이터베이스에서 발생하는 데이터 변경 사항을 추적하고 캡처하는 기술, 이를 통해 데이터베이스의 상태 변화(데이터의 삽입, 업데이트, 삭제 등)를 감지하고 이 변화를 다른 시스템으로 전파하거나 분석에 활용 가능 
		- CDC는 데이터 통합, 복제, 분석, 백업 등 다양한 용도로 사용
- 대용량 데이터 수집이 필요한 경우 변경사항을 복제하기 위해 MySQL 이진 로그 내용을 사용하는 것이 효과적
	- MySQL 이진 로그는 데이터베이스에서 수행된 모든 작업에 대한 기록을 보관
	- 구성된 방식에 따라 모든 테이블의 생성 또는 수정사항을 포함하여 모든 INSERT, UPDATE, DELETE 작업도 기록

---
##### TIP 사전 구축된 프레임워크 사용 고려
- 이진 로그 복제의 복잡성으로 인해 데이터를 수집하려면 오픈소스 프레임워크 또는 상용 제품을 고려하는 것이 좋음
---

- 데이터 웨어하우스가 MySQL 데이터베이스가 아닌 경우, 단순하게 내장된 MySQL 복제 기능을 사용할 수는 없음
- MySQL이 아닌 곳으로 데이터를 수집하기 위해 이진 로그를 사용하려면 다음과 같은 단계를 수행
	1. *MySQL 서버에서 이진로그를 활성화하고 구성한다.*
		- 이진 로그를 활성화하여 변경사항 추적 : `my.cnf` 파일에서 `log_bin` 설정을 활성화하고, 필요한 다른 옵션(예: `server_id`, `binlog_format`)을 구성
	2. *초기 전체 테이블 추출을 실행하고 로드한다.*
		- 이진 로그를 사용해 실시간 데이터를 동기화하기 전에, 데이터 웨어하우스의 초기 상태를 설정
		- 이 작업을 위해 MySQL의 모든 테이블을 데이터 웨어하우스로 내보내야 함
	3. *지속적으로 이진 로그를 추출한다.*
		- 초기 데이터를 로드한 후에는 MySQL 서버에서 이진 로그를 지속적으로 추출해야 함
		- 이진 로그에 기록된 데이터 변경사항을 실시간으로 데이터 웨어하우스로 전송
	4. *추출된 이진 로그를 데이터 웨어하우스로 변환하여 로드한다.*
		- 추출된 이진 로그는 MySQL 형식으로 저장되므로, 이를 데이터 웨어하우스에서 이해할 수 있는 형식으로 변환 필요

- 이진 로그를 수집하기 전에, 먼저 MySQL 데이터베이스의 현재 상태를 데이터 웨어하우스의 테이블에 반영해야 함
- 그 다음 이진 로그를 사용하여 후속 변경 사항을 수집
- 이를 위해 이진 로그 수집을 켜기 전에 추출한 소스 테이블에 LOCK을 설정한 뒤 해당 테이블에 대해 mysqldump를 실행한 다음 mysqldump 결과를 데이터 웨어하우스의 테이블에 로드해야 함
	- mysqldump : MySQL 및 MariaDB 데이터베이스에서 데이터를 백업하기 위해 사용하는 유틸리티로, 데이터베이스의 구조(테이블 스키마)와 데이터를 SQL 형식의 덤프 파일로 내보낼 수 있음

---
##### TIP 소스 시스템 소유자와의 논의
- MySQL 소스 시스템에서 이진 로그의 구성을 수정하기 위한 접근 권한은 종종 시스템 관리자에게만 부여됨
- 데이터를 수집하려는 데이터 엔지니어는 변경사항이 MySQL 서버뿐만 아니라 다른 시스템에 영향을 미칠 수 있으므로 이진 로그 구성을 변경하기 전에 항상 데이터베이스 소유자와 논의하고 함께 작업해야 함
---

[이진 로그 구성과 관련한 MySQL 데이터베이스 선행 작업]
1. *이진 로깅이 활성화 되어 있는지 확인*
	- 일반적으로 디폴트 값으로 활성화 됨
	- 데이터베이스에서 다음의 SQL 쿼리를 실행하여 확인 가능
	- (참고)이진 로깅 : 이진 로그를 생성하고 관리하는 과정
```sql
SELECT variable_value AS bin_log_status
FROM performance_schema.g]obal_variables
WHERE variabale_name='log_bin';
```
 → `log_bin`: MySQL 서버에서 이진 로깅 기능이 활성화되어 있는지 여부를 제어하는 시스템 변수로, 변수 값이 `ON`이면 이진 로깅이 활성화된 상태, `OFF`이면 비활성화된 상태

- 이진 로깅을 사용하도록 설정하면 다음이 표시됨
- 반환된 상태가 OFF인 경우 MySQL 설명서를 참조하여 해당 버전을 활성화
```
+-----------------------------+
| bin_log_status :: |
+-----------------------------+
| ON |
+-----------------------------+
```

2. *이진 로깅 형식이 적절히 설정되어 있는지 확인 (이진 로그 포맷 설정)*
	- 주로 사용되는 포맷 : MySQL 최신 버전에서는 **1) STATEMENT, 2) ROW, 3) MIXED** 형식이 지원됨
	- **STATEMENT**
		- 기능 : 이진 로그에 행을 삽입하거나 수정하는 행동들에 대해 SQL문 자체를 기록
		  → 이진 로그에는 `INSERT`, `UPDATE`, `DELETE`와 같은 SQL 쿼리문이 기록됨
		- 하나의 MySQL DB에서 다른 MySQL DB로 데이터를 복제하는 경우 STATMENT 형식이 유용함
		- 데이터 복제 시 모든 SQL문을 실행하여 DB 상태를 복제할 수 있음
		- 추출된 데이터는 타 플랫폼에서 실행되는 데이터 웨어하우스에 바인딩 되어야 하기 때문에, MySQL DB에서 생성된 SQL문이 데이터 웨어하우스와 호환되지 않을 수 있음
	- **ROW**
		- 기능 : 테이블의 행에 대한 모든 변경사항이 SQL문이 아니라 행 자체의 데이터로 이진 로그 행에 표시됨 (기본 형식)
		  → 변경된 행의 데이터가 이진 로그에 직접 저장됨
		- 데이터의 실제 변경 내용을 기록하므로 복제 및 데이터 일관성 유지가 더 정확함
		- 데이터 변경 작업 시 변경된 행의 데이터를 직접 이진 로그에 저장하기 때문에 로그 파일의 크기가 커질 수 있으며, 행 데이터가 저장되기 때문에 디스크 공간을 더 많이 소모함
	- **MIXED**
		- 기능 : 이진 로그에 STATEMENT 형식 레코드와 ROW 형식 레코드를 모두 기록
		- 추후 ROW 데이터만 걸러낼 수 있지만, 이진 로그를 다른 용도로 사용하지 않는 한 결국 디스크 공간을 추가로 사용하게 되기 때문에 MIXED를 활성화 할 필요는 없음

- 다음 SQL 쿼리를 실행하여 현재 이진 로그 형식을 확인
```sql
SELECT variable_value AS bin_log_format
FROM performance_schema.g]obal_variables
WHERE variabale_name='binlog_format';
```

- 현재 활성화된 형식을 반환
```
+-----------------------------+
| bin_log_status :: |
+-----------------------------+
| ON |
+-----------------------------+
```

- 이진 로그 형식과 기타 설정은 MySQL DB 인스턴스 관련 my.cnf 파일에서 설정됨
- 파일을 열면 다음과 같은 행이 표시됨
```
[mysqld]
binlog_format=row
```

- 이진 로깅이 ROW 형식으로 사용되도록 설정되었으므로 관련 정보를 추출하여 데이터 웨어하우스에 로드할 파일에 저장하는 프로세스를 구축할 수 있음
- ROW 형식 이벤트 세 가지 유형
	- **WRITE_ROWS_EVENT**
	- **UPDATE_ROWS_EVENT**
	- **DELETE_ROWS_EVENT**

[이진 로그에서 이벤트 가져오기]
- (참고)이진 로그에서 이벤트를 가져오는 과정과 관련된 코드는 다음과 같은 기능을 수행
	1. MySQL 서버에 연결하여 이진 로그를 읽는 설정을 구성
	2. BinLogStreamReader 클래스를 사용하여 이진 로그를 스트리밍하고, **DELETE_ROWS_EVENT**, **WRITE_ROWS_EVENT**, **UPDATE_ROWS_EVENT** 이벤트를 읽음
	3. 이벤트 출력 및 스트리밍 종료 후 리소스를 정리

- 오픈소스 파이썬 라이브러리(python-mysql-replication) 활용
```
(env) $pip install mysql-replication
```

- 이진 로그 출력 방식을 알아보기 위해 DB에 연결하고 이진 로그 읽어보기
	[예제]
	- 앞부분에 나왔던 예제에 대해 pipeline.conf 파일에 추가된 MySQL 연결 정보를 사용
	- MySQL 서버의 기본 이진 로그 파일에서 읽음
	- 기본 이진 로그 파일 이름 및 경로는 MySQL DB의 `my.cnf` 파일에 저장된 `log_bin` 변수에 설정됨
	- 이진 로그가 시간(매일 또는 매시간)에 따라 순환되는 경우,
		- MySQL 관리자가 선택한 로그 순환 방법 및 파일 이름 지정 체계에 따라 파일 경로를 결정하고
		- BinLogStreamReader 인스턴스를 만들 때 해당 경로를 log_file 매개변수에 값으로 전달해야 함
```python
from pymysqlreplication import BinLogStreamReader
from pymysqlreplication import row_event
import configparser
import pymysqlreplication

# MySQL 연결 정보 가져옴
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
hostname = parser.get("mysql_config", "hostname")
port = parser.get("mysql_config", "port")
username = parser.get("mysql_config", "username")
password = parser.get("mysql_config", "password")

mysql_settings = {
	"host": hostname,
	"port": int(port),
	"user": username,
	"passwd":password
}

b_stream = BinLogStreamReader(
			connection_settings = mysql_settings,
			server_id=100,
			only_events=[row_event.DeleteRowsEvent,
						row_event.WriteRowsEvent,
						row_event.UpdateRowsEvent]
			)
for event in b_stream:
	evnet.dump()

b_stream.close()
```

- 인스턴스화된 BinLogStreamReader 개체는 pipeline.conf 파일에 지정된 MySQL DB에 연결하고 특정 이진 로그 파일을 읽음 
- `resume_stream=True` 설정과 `log_pos` 값은 지정된 지점에서 이진 로그를 읽기 시작함
	→ 예제에서는 1400번 위치
	→ `resume_stream=True`는 `mysql-replication` 라이브러리에서 BinLogStreamReader를 사용하여 이진 로그를 읽을 때, 이전에 중단된 위치에서 계속해서 로그를 읽도록 설정하는 옵션
- BinLogStreamReader에게 DeleteRowsEvent, WriteRowsEvent, UpdateRowsEvent만 읽도록 지시
- 스크립트는 모든 이벤트를 반복하고 사람이 읽을 수 있는 형식으로 출력
- Orders 테이블이 있는 DB의 경우 다음과 같은 항목이 출력됨
```
=== WriteRowsEvnet ===
Date: 2020-06-01 12:00:00
Log position: 1400
Event size: 30
Read bytes: 20
Talbe: orders
Affected columns: 3
Changed rows: 1
Values:
--
* OrderId : 1
* OrderStatus : Backordered
* LastUpdated : 2020-06-01 12:00:00

=== UpdateRowsEvnet ===
Date: 2020-06-09 12:00:25
Log position: 1401
Event size: 56
Read bytes: 15
Talbe: orders
Affected columns: 3
Changed rows: 1
Affected columns: 3
Values:
--
* OrderId : 1 => 1
* OrderStatus : Backordered => Shipped
* LastUpdated : 2020-06-01 12:00:00 => 2020-06-09 12:00:25
```
  → OrderId 1의 INSERT 및 UPDATE를 나타내는 두 가지 이벤트가 있음(표 4-3 참조)

- 예제에서 두 순차적 이진 로그 이벤트는 며칠 간격으로 발생하지만, 실제 환경에서는 두 이벤트 사이에 DB 변경사항을 모두 나타내는 수많은 이벤트가 존재
- log_pos 값 : BinLogStreamReader의 시작 위치를 알려주며, 다음 추출이 실행될 때 선택할 위치를 추적하기 위해 테이블에 저장해야 하는 값
	- 추출이 시작될 때 읽고 쓸 수 있는 데이터 웨어하우스의 로그 테이블에 값을 저장하고 종료 시 최종 이벤트의 위치 값을 저장하는 것이 가장 좋음

| Orderld | OrderStatus | LastUpdated         |
| ------- | ----------- | ------------------- |
| 1       | Backordered | 2020-06-01 12:00:00 |
| 1       | Shipped     | 2020-06-09 12:00:25 |
[표4-3] 데이터 웨어하우스에 있는 모든 버전의 OrderId 1

- 예제 코드를 통해 이벤트를 사람이 읽을 수 있는 형식으로 표시하지만, 출력을 데이터 웨어하우스로 쉽게 로드하려면 다음의 작업 수행 필요
	1. *데이터를 다른 형식으로 구문 분석(parsing)하고 기록한다.*
		- 로딩을 단순화하기 위해 다음 예제에서는 CSV 파일의 행에 각 이벤트를 기록
		- 이진 로그를 데이터 웨어하우스로 로드하려면, 이벤트를 적절한 형식(CSV)으로 구문 분석하여 기록해야 함
	2. *추출 및 로드하려는 테이블 당 하나의 파일을 작성한다.*
		  - 예제의 이진 로그에는 Orders 테이블과 관련된 이벤트만 포함되지만 실제 이진 로그에는 다른 테이블과 관련된 이벤트도 포함될 가능성이 높음
		  - 각 테이블에 대한 별도의 파일을 생성하거나, 예제처럼 단순성을 위해 하나의 파일에 모든 관련 이벤트를 기록할 수 있음
	- 1번 사항 해결을 위해 .dump() 함수를 사용하는 대신 이벤트 속성을 구문 분석하여 CSV 파일에 씀
	- 2번의 경우 각 테이블에 대한 파일을 작성하는 대신 단순성을 위해 orders_extract.csv 파일에 Orders 테이블과 관련된 이벤트만 기록

- 구현된 전체 추출 프로세스에서 예제 코드를 수정하여 테이블별 이벤트를 그룹화하고, 각 테이블의 변경내용을 별도의 파일로 저장
- 마지막 단계에서는 CSV 파일을 S3 버킷에 업로드하여 데이터 웨어하우스에 로드 가능
```python
from pymysqlreplication import BinLogStreamReader
from pymysqlreplication import row_event
import configparser
import pymysqlreplication
import csv
import boto3

# MySQL 연결 정보 가져옴
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
hostname = parser.get("mysql_config", "hostname")
port = parser.get("mysql_config", "port")
username = parser.get("mysql_config", "username")
password = parser.get("mysql_config", "password")

mysql_settings = {
	"host": hostname,
	"port": int(port),
	"user": username,
	"passwd":password
}

b_stream = BinLogStreamReader(
			connection_settings = mysql_settings,
			server_id=100,
			only_events=[row_event.DeleteRowsEvent,
						row_event.WriteRowsEvent,
						row_event.UpdateRowsEvent]
			)

order_events = []

for binlogevent in b_stream:
	for row in binlogevent.rows:
		if binlogevent.table == 'orders':
		event = {}
		if isinstance(
				binlogevent,row_event.DeleteRowsEvent
			):
			event["action"] = "delete"
			event.update(row["values"].items())
		elif isinstance(
			binlogevent,row_event.UpdateRowsEvent
			):
			event["action"] = "update"
			event.update(row["values"].items())
		
		order_events.append(event)

b_stream.close()

keys = order_events[0].keys()
local_filename = 'orders_extract.csv'
with open(
		local_filename,
		'w',
		newline='') as output_file:
	dict_writer = csv.DictWriter(
				output_file, keys, delimiter='|')
	dict_writer.writerows(order_events)

# aws_boto_credentials 값을 로드
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
access_key = parser.get(
				"aws_boto_credentials",
				"access_key")
secret_key = parser.get(
				"aws_boto_credentials",
				"secret_key")

bucket_name = parser.get(
				"aws_boto_credentials", 
				"bucket_name")

s3 = boto3.client(
	's3', 
	aws_access_key_id=access_key, 
	aws_secret_access_key=secret_key)

s3_file = local_filename

s3.upload_file(
	local_filename, 
	bucket_name, 
	s3_file)
```

- 실행 후 orders_extract.csv는 다음과 같이 표시됨
```
insert|1|Backordered|2020-06-01 12:00:00
update|1|Shipped|2020-06-09 12:00:25
```
 
- 결과 CSV 파일의 형식은 빠른 로딩에 최적화되어 있음
  (추출된 데이터 관련은 5~6장에서 진행됨)
