- # 데이터 변환하기
	- **ELT 패턴과 데이터 변환**
		- ELT 패턴에서 데이터 레이크 또는 데이터 웨어하우스에 저장된 데이터는 파이프라인의 다음 단계인 데이터 변환 과정을 거친다
		- 데이터 변환에는 비즈니스 컨텍스트 및 논리를 엮은 데이터 모델링이 포함
	- **파이프라인의 목적**
		- 파이프라인의 주요 목적은 비즈니스 통찰력이나 분석을 생성하는 것
		- 이를 위해 비즈니스 분석 외에도 데이터 모델링을 포함한 추가 변환이 필요
		- 데이터 모델링은 데이터를 분석할 수 있도록 정형화하는 과정
	- **데이터 엔지니어와 분석가의 역할**
		- 데이터 엔지니어나 파이프라인에서 비즈니스 분석을 위한 변환 작업은 SQL을 주로 사용
		- 다양한 라이브러리와 도구들이 사용
		- 이는 데이터 수집 및 변환 작업을 간소화하고 최적화하기 위해 필요한 기술이다
	- **샘플 코드**
		- 거의 모든 데이터 파이프라인은 비즈니스 문제 해결을 위한 변환 작업을 수반하며, 샘플 코드는 SQL 기반으로 작성
		- 파이썬을 통해 복잡한 데이터 수집 및 변환을 보다 긴밀하게 수행할 수 있습니다.
- # 비문맥적 변환
	- ## EtLT
		- 소문자 t는 다음과 같은 일부 비문맥 데이터 변환
			- 테이블의 중복 레코드 제거
			- URL 매개변수를 개별 구성요소로 분석
	- 데이터 수집 후에 변환하는 것 ELT에 비해 수집의 일부로 이러한 변환을 수행하는 것(EtLT)이 적절한 경우에 대해 설명
	- ## 테이블에서 레코드 중복 제거
		- 데이터 웨어 하우스에 수집된 데이터 테이블에 중복 레코드가 존재할 수 있다
			- 증분 데이터 수집에서 실수로 이전 수집 시간 창과 겹치거나 이전 실행에서 이미 수집된 일부 레코드를 선택한 경우
			- 원본 시스템에서 중복 레코드가 실수로 생성된 경우
			- 나중에 채워진(backfiled) 데이터가 테이블로 로드된 후속 데이터와 겹치는 경우
		- 중복 레코드를 확인, 제거는 SQL 쿼리로 수행
			![](https://i.imgur.com/EXFqgik.png)
			- 5개의 레코드중 2개 중복
			- 2, 4 번째 동일
			```SQL
			CREATE TABLE Orders (
			  OrderId int,
			  OrderStatus varchar(30),
			  LastUpdated timestamp
			);
			
			INSERT INTO Orders
			  VALUES(1,'Backordered', '2020-06-01');
			INSERT INTO Orders
			  VALUES(1,'Shipped', '2020-06-09');
			INSERT INTO Orders
			  VALUES(2,'Shipped', '2020-07-11');
			INSERT INTO Orders
			  VALUES(1,'Shipped', '2020-06-09');
			INSERT INTO Orders
			  VALUES(3,'Shipped', '2020-07-12');
			```
		- ### 테이블 중복 레코드 식별
			- SQL -> `GROUP BY` or `HAVING`
			```SQL
			SELECT OrderId,
				OrderStatus,
				LastUpdated,
				COUNT(*) AS dup_count
			FROM Orders
			GROUP BY OrderId, OrderStatus, LastUpdated
			HAVING COUNT(*) > 1;
			```
			- 반환 값
				![](https://i.imgur.com/ByaqJ7i.png)
		- ### 중복 레코드 제거
			- 데이터 베이스 최적화, SQL 구문의 기본 설정에 따라서 방법 결정
			1. 쿼리 시퀀스 사용
				- `DISTINCT` 문을 사용하여 원본 테이블의 복사본 생성
				- `DISTINCT` = 두 개의 중복 행이 하나로
				```
				CREATE TABLE distinct_orders AS
				SELECT DISTINCT OrderId,
				  OrderStatus,
				  LastUpdated
				FROM ORDERS;
				
				TRUNCATE TABLE Orders;
				
				INSERT INTO Orders
				SELECT * FROM distinct_orders;
				
				DROP TABLE distinct_orders;
				```
			2. 윈도우 기능(window function)
				- 중복 행 그룹화 -> 행번호 할당 -> 삭제할 행과 유지할 행 식별
				- `ROW_NUMBER` 사용하여 레코드 순위 지정
				- `PARTITION BY` 사용하여 열별 그룹화
				- 둘 이상의 일치 항목이 있는 모든 레코드 그룹에 1보다 큰 `ROW_NUMBER`가 할당
				```
				CREATE TABLE all_orders AS
				SELECT
				  OrderId,
				  OrderStatus,
				  LastUpdated,
				  ROW_NUMBER() OVER(PARTITION BY OrderId,
				                    OrderStatus,
				                    LastUpdated)
				    AS dup_count
				FROM Orders
				```
				- 쿼리 결과
				![](https://i.imgur.com/YsolLyZ.png)
				![](https://i.imgur.com/O9g6iLT.png)
				- 세 번째 행은 바로 위에 레코드 복제본
				- dup_count 값이 2
				```
				CREATE TABLE all_orders AS
				SELECT
				  OrderId,
				  OrderStatus,
				  LastUpdated,
				  ROW_NUMBER() OVER(PARTITION BY OrderId,
				                    OrderStatus,
				                    LastUpdated)
				    AS dup_count
				FROM Orders;
				
				TRUNCATE TABLE Orders;
				
				-- only insert non-duplicated records
				INSERT INTO Orders
				  (OrderId, OrderStatus, LastUpdated)
				SELECT
				  OrderId,
				  OrderStatus,
				  LastUpdated
				FROM all_orders
				WHERE
				  dup_count = 1;
				
				DROP TABLE all_orders;
				```
			1. 중복 제거된 레코드가 있는 테이블 만들기
			2. Orders  테이블 자르기
			3. 정리된 데이터세트를 Orders에 삽입
			![](https://i.imgur.com/LGpY8RV.png)
	- ## URL 파싱
		- URL의 세그먼트 분석하는것은 비즈니스 컨텍스트가 관련되지 않은 작업
		- URL에 변환 단계에서 구문 분석
		- 데이터베이스 테이블의 개별 열에 저장할 수 있는 구성요소가 많다
		- ex)`https://www.mydomain.com/page-name?utm_content=textlink&utm_medium=social&utm_source=twitter&utm_campaign=fallsale`
			1. 도메인 : `www.mydomain.com`
			2. URL : `/page-name`
			3. utm_content 매개변수 값 : `textlink`
			4. utm_medium 매개변수 값 : `social`
			5. utm_source 매개변수 값 : `twitter`
			6. utm_campaign 매개변수 값 : `fallsale`
		- TIP - UTM (Urchin Tracking Module) 매개변수
			- 마케팅 및 광고 캠페인을 추적하는 데 사용
			- URL 매개변수
			- 대부분의 플랫폼과 조직에서 공통적
		- ### URL 구문 분석
			- SQL, Python 모두 가능
			- 변환을 실행하는 시간, URL이 저장된 위치는 무엇을 사용할지 결정하는데 도움 된다
			- ex) EtLR 패턴을 따르고 소스에서 추출 후 데이터 웨어하우스의 테이블에 로드하기 전에 URL 구문 분석할 수 있다면 파이썬
		- #### Python
			`pip install urllib3`
			- urlsplit, parse_ps 함수 사용
			```python
			from urllib.parse import urlsplit, parse_qs
			
			url = """https://www.mydomain.com/page-name?utm_content=textlink&utm_medium=social&utm_source=twitter&utm_campaign=fallsale"""
			
			split_url = urlsplit(url)
			params = parse_qs(split_url.query)
			
			# 도메인
			parsed_url.append(split_url.netloc)
			
			# url 경로
			parsed_url.append(split_url.path)
			
			# url 매개변수
			parsed_url.append(params['utm_content'][0])
			parsed_url.append(params['utm_medium'][0])
			parsed_url.append(params['utm_source'][0])
			parsed_url.append(params['utm_campaign'][0])
			```
			- 실행 결과
				![](https://i.imgur.com/k6afqxl.png)
		- #### SQL
			```sql
			from urllib.parse import urlsplit, parse_qs
			import csv
			
			url = """https://www.mydomain.com/page-name?utm_content=textlink&utm_medium=social&utm_source=twitter&utm_campaign=fallsale"""
			
			split_url = urlsplit(url)
			params = parse_qs(split_url.query)
			parsed_url = []
			all_urls = []
			
			# 도메인
			parsed_url.append(split_url.netloc)
			
			# url 
			parsed_url.append(params['utm_content'][0])
			parsed_url.append(params['utm_medium'][0])
			parsed_url.append(params['utm_source'][0])
			parsed_url.append(params['utm_campaign'][0])
			
			all_urls.append(parsed_url)
			
			export_file = "export_file.csv"
			
			with open(export_file, 'w') as fp:
				csvw = csv.writer(fp, delimiter='|')
				csvw.writerows(all_urls)
			
			fp.close()
			```
			- SQL을 사용하여 데이터 웨어하우스에 이미 로드된 URL 구문 분석 어려움
			- Snowflake는 URL을 구성 요소로 구문 분석한 결과를 JSON 으로 변환하는 PARSE_URL 함수 제공
			- URL 구문 분석 결과
			```
			SELECT parse_url('https://www.mydomain.com/page-name?utm_content=textlink&utm_medium=social&utm_source=twitter&utm_campaign=fallsale');
			```
			![](https://i.imgur.com/3zKTzhg.png)
		- Redshift, 다른 데이터 웨어하우스 플랫폼 사용하는 경우 사용자 지정 문자열 구문 분석, 정규식을 사용해야함
		- Redshift 에는 REGEXP_SUBSTR 함수가 있다
		- 데이터 수집, 로드 하는 동안 파이썬 또는 다른 언어로 URL 구성 요소 구문 분석 하는게 좋다
- # 언제 변환? -> 수집 중? 수집 후?
	- 수집 프로세스(EtLT 패턴)의 일부로 실행하는것을 고려해야하는 이유
		1. 변환은 SQL 이외의 언어를 사용하여 수행하는것이 가장 쉽다
		2. 변환은 데이터 품질 문제를 해결한다
			- 파이프라인 초기 데이터 품질을 해결 하는것이 가장 좋다 (9장에서..)
	- 비지니스 로직과 관련된 변환의 경우 데이터 수집과 변환을 별도로 분리하는 것이 가장 좋다
	- 이러한 유형의 변환을 **데이터 모델링** 이라고 한다