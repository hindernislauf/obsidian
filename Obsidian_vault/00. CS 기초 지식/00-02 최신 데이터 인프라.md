# 데이터 소스의 다양성
 
 - 대부분 조직에는 여러 개의 데이터 소스 존재
   → 소유 중인 데이터 소스를 이용한 분석 작업 수행 가능

# 소스 시스템 소유권
 
 - 데이터 소스
	 - 조직이 구축하고 소유한 소스 시스템
		 - 예) 자사 앱에서 생성되는 데이터를 PostgresSQL 데이터베이스에 저장
	 - 타사 도구 · 공급업체
		 - 예) 타사 웹 분석 도구(Google Analytics)를 사용한 웹 사이트 사용 추적
	 - 데이터 파이프라인
		 - 두 소스 모두에서 데이터를 수집[^1]하면서 시작 
		   → 데이터 분석으로 종료
		![|400](https://i.imgur.com/pjUKNkq.png)
	[^1]데이터 수집: 한 소스에서 데이터를 추출하여 다른 소스로 로드하는 것을 의미

- 소스 시스템 소유권의 중요성
	1. 타사 데이터 소스의 데이터에 액세스할 때, 액세스 방법의 제한 가능성
		- 예) 대다수 솔루션 공급업체: REST API 제공 / SQL 데이터베이스 미제공
		- 액세스 가능한 데이터와 세부 수준을 사용자 환경에 맞추기 어려움
	2. 내부 구축 시스템의 데이터에 액세스할 때, 자유로운 액세스 방법 제공 가능
		- 사용자 필요에 맞추어 데이터 형태 정의 가능
		  → 분석 팀에 더 많은 분석 기회 제공 가능
		- 시스템 구축 시, 데이터 수집을 고려한 설계 여부는 별개의 문제임
			- 예) 데이터 수집이 의도하지 않은 시스템 부하 야기 문제, 데이터의 점진적(incremental) 로드 가능 여부 등 다양한 과제 발생
			- 제한적인 리소스로 소스 시스템 소유 엔지니어 팀과 협력이 어려울 수 있음
# 수집 인터페이스 및 데이터 구조
- 데이터 엔지니어가 새로운 데이터 수집 구축 시, 
  소스 데이터를 얻는 방법 · 형식을 알아야 함
1. 데이터 인터페이스
	- 애플리케이션 뒤에 위치한 데이터베이스
		- 예) Postgres, MySQL 등 backend database
	- 시스템 상단의 추상화 계층(예: REST API)
	- 스트림 처리 플랫폼(예: Apache Kafka)
	- Log, CSV(Comma Separated Value), 기타 플랫 파일이 저장되는 
	  NFS(Network File System) 또는 Cloud Storage Bucket
	- 데이터 웨어하우스 / 데이터 레이크
	- HDFS / HBase 데이터베이스의 데이터
2. 데이터 구조
	- REST API - JSON(JavaScript Object Notation)
	- MySQL DB의 잘 구성된 데이터
	- MySQL DB 테이블의 column 내 JSON
	- 반정형화된 로그 데이터
	- CSV, FWF(Fixed Width File, 고정 폭 형식), 기타 플랫 파일 형식
	- 플랫 파일 - JSON
	- Kafka 스트림 출력
- 각 인터페이스, 데이터 구조
  → 각각의 도전 과제와 기회 존재
	- 잘 구성된 데이터
		- 작업하기 가장 쉬운 편 / 일반적으로 애플리케이션 · 웹 사이트를 위해 정형화
		- 파이프라인 상 클렌징, 변환 작업 등 추가 단계 필요 가능성 존재
	- 반정형 데이터(JSON)
		- 장점: 속성-값 구조, 객체의 중첩(nesting) 구조
		- 단점: 데이터세트 내 데이터 구조의 동일성 보장 불가 / 데이터의 유연성이 커지면 필요 데이터 처리 방법도 증가
	- 비정형 데이터
		- 일부 분석 작업에 흔히 사용(예: NLP, CV, 웹 크롤링 등)
# 데이터 사이즈
 - 데이터 사이즈와 가치는 비례하지 않음
 - 파이프라인의 각 단계 설계 시, 데이터 사이즈 고려 필요
# 데이터 클렌징 작업과 유효성 검사
- 소스 데이터의 한계와 결함을 이해하고 파이프라인의 적절한 부분에서 해결하는 것이 중요
- '지저분한 데이터' 공통적 특성
	- 중복되거나 모호한 레코드
	- 고립된 레코드
	- 불완전하거나 누락된 레코드
	- 텍스트 인코딩 오류
	- 일치하지 않는 형식(예: 대시(-)가 있는 전화번호와 없는 전화번호)
	- 레이블이 잘못되었거나 레이블이 지정되지 않은 데이터
- 데이터 클렌징과 유효성을 보장하기 위한 주요 특성과 접근 방식
	- 최악을 가정하고 최상을 기대하라
		- 입력 데이터세트에 수많은 유효성 및 일관성 문제가 있지만,
		  깨끗한 출력을 위해 데이터를 식별하고 정리하는 파이프라인을 구축한다고 가정
	- 가장 적합한 시스템에서 데이터를 정리하고 검증하라
		- 때로는 데이터를 원본 그대로 데이터 레이크에 로드 후 나중에 파이프라인에서 정형화 및 정리하는 방식이 좋을 수 있음(ETL → ELT)
		- 데이터 클렌징과 검증 프로세스 → 올바른 작업에 올바른 도구 사용
	- 자주 검증하라
		- 파이프라인 종료 단계에서 데이터 검증 시, 문제 발생 단계 파악이 어려움
# 소스 시스템의 지연 시간 및 대역폭
- 파이프라인의 데이터 추출 단계는 소스 시스템의 소유자가 불만이 생기게 할 수 있음
- 불만 요인
	- API 속도 제한
	- 연결 시간 초과
	- 느린 다운로드
	- 시스템에 대한 부담