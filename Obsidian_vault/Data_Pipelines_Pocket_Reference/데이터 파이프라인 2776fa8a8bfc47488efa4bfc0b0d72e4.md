## MySQL 데이터베이스에서 데이터 추출

### MySQL 데이터베이스에서 데이터 추출 방법

- SQL을 사용한 전체 또는 증분 추출
    - 구현이 간단
    - 자주 변경되는 대규모 데이터세트에서는 확장성이 떨어짐
    - 전체 추출과 증분 추출 사이에도 트레이드오프가 있음(다음 파트)
- 이진 로그(binlog) 복제
    - 구현이 복잡
    - 스트리밍 데이터 수집을 수행하는 경로
    - 원본 테이블의 변경되는 데이터 볼륨이 크거나
    - MySQL 소스에서 데이터를 더 자주 수집해야 하는 경우 적합함

### 설치 방법

1. 로컬 시스템에 설치
2. AWS에서 Amazon RDS를 생성(MySQL 인스턴스 관리)
    1. 샘플을 학습, 작업 → 데이터베이스를 공개적으로 액세스할 수 있게 설정
    2. 프로덕션 설정 → 강력한 보안이 필요하기에 Amazon RDS 보안 모범 사례 따르기 
    3. 단, 비용 발생

```sql
# 테이블 생성
CREATE TABLE Orders ( 
	OrderId int,
	OrderStatus varchar(30), 
	LastUpdated timestamp
);

# 행 삽입
INSERT INTO Orders
	VALUES (1, 'Backordered', '2020-06-01 12:00:00'); 
INSERT INTO Orders
	VALUES (1, 'Shipped', '2020-06-09 12:00:25');
INSERT INTO Orders
	VALUES (2, 'Shipped', '2020-07-11 3:05:00'); 
INSERT INTO Orders
	VALUES (1, 'Shipped', '2020-06-09 11:50:00'); 
INSERT INTO Orders
	VALUES (3, 'Shipped', '2020-07-12 12:00:00');
```

# 전체 또는 증분 MySQL 테이블 추출

전체 추출과 증분 추출은 데이터베이스에서 데이터를 처리하거나 분석할 때 사용하는 두 가지 주요 접근 방식입니다. 각각의 접근 방식은 장단점이 있으며, 선택할 때는 여러 가지 트레이드오프를 고려해야 합니다.

### 전체 추출 (Full Extraction)

```sql
SELECT *
FROM Orders;
```

**장점:**

1. **완전성**: `전체 데이터를 새로 추출`하므로 데이터의 일관성과 정확성을 보장할 수 있습니다. 이전 데이터 상태와 관계없이 항상 최신 상태를 제공합니다. 이전 데이터는 삭제합니다.
2. **단순성**: 데이터베이스 쿼리나 로직이 간단해질 수 있습니다. 모든 데이터를 한 번에 가져오면 별도의 추적이나 비교 작업이 필요 없습니다.

**단점:**

1. **성능**: 대량의 데이터를 매번 새로 추출해야 하므로 데이터베이스와 네트워크에 큰 부하를 줄 수 있습니다. 데이터가 많을수록 성능에 더 큰 영향을 미칩니다.
2. **시간**: 전체 데이터를 매번 추출하고 처리하는 데 시간이 많이 걸릴 수 있습니다.

### 증분 추출 (Incremental Extraction)

```sql
SELECT *
FROM Orders
WHERE LastUpdated > {{ last_extraction_run }};
```

- `LastUpdated`: 데이터베이스 테이블의 데이터가 마지막으로 업데이트된 시점을 나타내는 필드입니다. 이 필드는 일반적으로 `TIMESTAMP` 또는 `DATETIME` 데이터 타입을 가집니다.
- `{{ last_extraction_run }}`: 이 값은 템플릿 문법을 사용하여 외부에서 주입되는 변수입니다. 추출 작업의 최근 실행 시간을 나타내는 타임스탬프입니다. 이 값은 쿼리 실행 시점에 동적으로 설정됩니다.
    
    ```sql
    SELECT MAX(LastUpdated)
    FROM warehouse.Orders;
    ```
    
- 마지막 데이터 추출 이후에 업데이트된 레코드만을 가져오려면 이 조건을 사용합니다.
- 변경할 수 없는 데이터(레코드를 삽입 가능, 업데이트 불가)가 포함된 테이블의 경우, Last updated 열 대신 레코드가 생성된 시간에 대한 타임스탬프를 사용

**장점:**

1. **성능**: 데이터의 `변화만 추출`하므로, 전체 데이터셋을 새로 추출하는 것보다 효율적입니다. 특히 데이터가 자주 변경되지 않는 경우 유리합니다.
2. **시간**: 데이터 변경 사항만 처리하므로, 전체 추출보다 빠르게 완료될 수 있습니다.

**단점:**

1. **복잡성**: 증분 추출 로직이 더 복잡할 수 있습니다. 변경된 데이터만 추적하고 비교하는 메커니즘이 필요하며, 이를 위한 추가적인 관리가 필요합니다.
2. **일관성 문제**: 변경 사항을 정확하게 추적하지 못하거나 처리 과정에서 데이터 일관성 문제가 발생할 수 있습니다. 예를 들어, 데이터 누락이나 중복이 발생할 수 있습니다.
    1. 삭제된 행은 캡처되지 않는다.
    2. 원본 테이블에는 마지막으로 업데이트된 시간에 대한 신뢰할 수 있는 타임스탬프가 있어야 한다.

### 트레이드오프 요약

- **성능과 시간**: 증분 추출은 성능과 시간 면에서 유리하지만, 전체 추출은 데이터의 일관성과 정확성을 보장합니다.
- **복잡성과 관리**: 증분 추출은 더 복잡한 로직과 관리가 필요하며, 전체 추출은 상대적으로 단순하지만 성능적인 제약이 클 수 있습니다.
- **데이터의 일관성**: 전체 추출은 데이터의 일관성을 유지하는 데 유리하지만, 증분 추출은 일관성을 유지하는 데 더 많은 주의가 필요합니다.

결론적으로, 전체 추출과 증분 추출 중 어떤 방법을 선택할지는 데이터의 크기, 업데이트 빈도, 성능 요구사항, 시스템의 복잡도 등에 따라 달라질 수 있습니다.

### 파이썬 스크립트에 의해 트리거되는 SQL 쿼리를 사용하여 구현

1. 라이브러리 설치

```sql
(env)  $ pip install pymysql
```

1. 연결 정보를 저장하기 위해 pipeline.conf 파일 업데이트

```sql
[mysal_config] 
hostname = my_host.com 
port = 3306
username = my_user_name
password = my_password 
database = db_name
```

1. `extract_mysql full.py`라는 새 파이썬 스크립트를 생성
    1. 수집 로드 단계에서 데이터 웨어하우스로 추출된 데이터를 쉽게 정형화하고 사용 가능

```sql
import pymysql 
import csv
import boto3          # S3 버킷에 업로드 위해
import configparser
```

1. MySQL 데이터베이스에 대한 연결을 초기화

```sql
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
hostname = parser.get("mysal_config", "hostname")
port = parser.get("mysal_config", "port")
username = parser.get("mysal_config", "username")
dbname = parser.get("mysal_config", "database")
password = parser.get("mysal_config", "password")

conn = pymysql.connect(host=hostname,
	user=username, 
	password=password, 
	db=dbname, 
	port=int(port))
	
if conn is None:
	print("Error connecting to the MySQL database") 
else:
	print("MySQL connection established!")
```

1. Orders 테이블의 전체 추출

```sql
m_query = "SELECT * FROM Orders;" 
local_filename = "order_extract.csv"

m_cursor = conn.cursor() 
m_cursor.execute(m_query)
results = m_cursor.fetchall()

with open(local_filename, 'w') as fp: 
	csv_w = csv.writer(fp, delimiter='|') 
	csv_w.writerows(results)
fp.close() 
m_cursor.close() 
conn.close()
```

1. CSV 파일을 S3 버킷에 업로드

```sql
# aws_boto_credentials 값을 로드
parser =configparser.ConfigParser() 
parser.read("pipeline.conf")
access_key = parser.get("aws_boto_credentials", "access_key") 
secret_key = parser.get("aws_boto_credentials", "secret_key") 
bucket_name = parser.get("aws_boto_credentials", "bucket_name")

S3 = boto3.client('53', 
aws_access_key_id=access_key, 
aws_secret_access_key=secret_key)

S3_file = local_filename

S3.upload_file(local_filename, bucket_name, S3_file)
```

1. 스크립트 실행

```sql
(env) $ python extract_mysql_full.py
```

### 데이터 증분 추출을 위한 스크립트 변경사항

1. 스크립트 시작점으로 `extract_mysql_full.py` 파일을 복사하여 `extract_mysql_incremental.py` 사본에서 시작
    
    ⇒ 소스 데이터베이스에 부담을 주며 프로덕션 쿼리 실행도 차단할 수 있기 때문에 복제본 설정을 고려
    
2. Orders 테이블에서 추출한 마지막 레코드의 타임스탬프를 찾기
    1. Redshift 클러스터와 상호 작용하려면, psycopg2 라이브러리를 설치
    2. Redshift 클러스터에 연결하고 쿼리
    
    ```sql
    import psycopg2
    
    # Redshift db connection 정보를 가져옴 
    parser = configparser.ConfigParser() 
    parser.read("pipeline.conf")
    dbname = parser.get("aws_creds", "database")
    user = parser.get("aws_creds", "username") 
    password = parser.get ("aws_creds", "password") 
    host = parser.get("aws_creds", "host")
    port = parser.get ("aws_creds", "port")
    
    # Redshi ft 클러스터에 연결 
    rs_conn = psycopg2. connect(
    	"dbname=" + dbname
    	+ "user=" + user
    	+ "password=" + password 
    	+ "host=" + host
    	+ "port=" + port)
    
    rs_sql = "'SELECT COALESCE(MAXLastUpdated),
    	"1900-01-01')
    	FROM Orders;""" 
    rs_cursor = rs_conn.cursor()
    rs_cursor.execute(rs_sql) 
    result = rs_cursor.fetchone()
    
    # 오직 하나의 레코드만 반환됨 
    last_updated_warehouse = result[0]
    
    rs_cursor.close() 
    rs_conn.commit()
    ```
    
    - `last_updated_warehouse` 에 저장된 값을 사용하여 MySQL 데이터베이스에서 실행되는 추출 쿼리를 수정
    - 이전 추출 작업 실행 이후 Orders 테이블에서 업데이트된 레코드만 가져올 수 있음
    - (42p) ~44
    - 새 쿼리에는 last_update_warehouse 값에 대해 %s로 표시되는 자리 표시자가 포함되어 있다. 그러면 값이 튜플(데이터 집합을 저장하는데 사용되는 데이터 유형)로 cursor의 .execute() 함수에 전달된다 . 이는 SQL 주입(injection)을 방지하기 위해 SQL 쿼리에 매개변수를 추가해 주는 적절하고 안전한 방법이다. 다음은 MySQL 데이터베이스에서 SQL 쿼리를 실행해주는 업데이트된 코드다.
    
    ```python
    m_query = """SELECT * 
    	FROM Orders
    	WHERE LastUpdated > %s;""" 
    local_filename = "order_extract.csv"
    
    m_cursor = conn.cursor()
    m_cursor.execute(m_query, (last_updated_warehouse,))
    ```
    
- `extract_mysql_full.py`
    
    ```python
    import pymysql
    import csv
    import boto3
    import configparser
    
    parser = configparser.ConfigParser()
    parser.read("pipeline.conf")
    hostname = parser.get("mysql_config", "hostname")
    port = parser.get("mysql_config", "port")
    username = parser.get("mysql_config", "username")
    dbname = parser.get("mysql_config", "database")
    password = parser.get("mysql_config", "password")
    
    conn = pymysql.connect(host=hostname,
            user=username,
            password=password,
            db=dbname,
            port=int(port))
    
    if conn is None:
      print("Error connecting to the MySQL database")
    else:
      print("MySQL connection established!")
    
    m_query = "SELECT * FROM Orders;"
    local_filename = "order_extract.csv"
    
    m_cursor = conn.cursor()
    m_cursor.execute(m_query)
    results = m_cursor.fetchall()
    
    with open(local_filename, 'w') as fp:
      csv_w = csv.writer(fp, delimiter='|')
      csv_w.writerows(results)
    
    fp.close()
    m_cursor.close()
    conn.close()
    
    # load the aws_boto_credentials values
    parser = configparser.ConfigParser()
    parser.read("pipeline.conf")
    access_key = parser.get("aws_boto_credentials", "access_key")
    secret_key = parser.get("aws_boto_credentials", "secret_key")
    bucket_name = parser.get("aws_boto_credentials", "bucket_name")
    
    s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)
    
    s3_file = local_filename
    
    s3.upload_file(local_filename, bucket_name, s3_file)
    ```
    
- 이 코드는 MySQL 데이터베이스에서 데이터를 추출하여 CSV 파일로 저장하고, 해당 파일을 AWS S3 버킷에 업로드하는 작업을 수행합니다. 아래는 각 단계에 대한 설명입니다.
    1. **MySQL 데이터베이스 연결 정보 로딩**:
        
        ```python
        import configparser
        
        parser = configparser.ConfigParser()
        parser.read("pipeline.conf")
        hostname = parser.get("mysql_config", "hostname")
        port = parser.get("mysql_config", "port")
        username = parser.get("mysql_config", "username")
        dbname = parser.get("mysql_config", "database")
        password = parser.get("mysql_config", "password")
        
        ```
        
        - `configparser`를 사용하여 `pipeline.conf` 파일에서 MySQL 데이터베이스 연결 정보를 읽어옵니다.
        - 연결 정보는 호스트네임, 포트, 사용자 이름, 데이터베이스 이름, 비밀번호를 포함합니다.
    2. **MySQL 데이터베이스 연결 및 쿼리 실행**:
        
        ```python
        import pymysql
        
        conn = pymysql.connect(host=hostname, user=username, password=password, db=dbname, port=int(port))
        
        if conn is None:
          print("Error connecting to the MySQL database")
        else:
          print("MySQL connection established!")
        
          m_query = "SELECT * FROM Orders;"
        
        m_cursor = conn.cursor()
        m_cursor.execute(m_query)
        results = m_cursor.fetchall()
        
        ```
        
        - `pymysql` 라이브러리를 사용하여 MySQL 데이터베이스에 연결합니다.
        - 데이터베이스 연결이 성공하면 "MySQL connection established!" 메시지를 출력합니다.
        - `"SELECT * FROM Orders;"` 쿼리를 실행하여 `Orders` 테이블의 모든 데이터를 가져옵니다.
    3. **CSV 파일로 데이터 저장**:
        
        ```python
        import csv
        
        local_filename = "order_extract.csv"
        
        with open(local_filename, 'w') as fp:
          csv_w = csv.writer(fp, delimiter='|')
          csv_w.writerows(results)
        
        fp.close()
        m_cursor.close()
        conn.close()
        
        ```
        
        - 가져온 데이터를 `order_extract.csv`라는 파일에 저장합니다.
        - CSV 파일을 열고, 데이터를 `|`로 구분된 형식으로 저장합니다.
    4. **AWS S3에 파일 업로드**:
        
        ```python
        import boto3
        
        parser = configparser.ConfigParser()
        parser.read("pipeline.conf")
        access_key = parser.get("aws_boto_credentials", "access_key")
        secret_key = parser.get("aws_boto_credentials", "secret_key")
        bucket_name = parser.get("aws_boto_credentials", "bucket_name")
        
        s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)
        
        s3_file = local_filename
        
        s3.upload_file(local_filename, bucket_name, s3_file)
        
        ```
        
        - `configparser`를 다시 사용하여 AWS S3에 접근하기 위한 인증 정보와 버킷 이름을 읽어옵니다.
        - `boto3` 라이브러리를 사용하여 AWS S3 클라이언트를 생성하고, CSV 파일을 지정된 S3 버킷에 업로드합니다.
    
    ### 요약
    
    1. **MySQL 데이터베이스에서 데이터를 추출**: `Orders` 테이블의 데이터를 쿼리합니다.
    2. **CSV 파일로 저장**: 추출된 데이터를 CSV 파일로 기록합니다.
    3. **S3에 업로드**: 생성된 CSV 파일을 AWS S3 버킷에 업로드합니다.
    
    이 코드는 데이터 파이프라인의 한 부분으로, 데이터베이스에서 정보를 추출하여 저장하고, 클라우드 스토리지로 백업하는 과정을 자동화합니다.
    
- 파이썬에서 데이터베이스와 상호작용할 때 `cursor`는 SQL 쿼리를 실행하고 결과를 관리하는 데 중요한 역할을 합니다. 아래는 `cursor`의 주요 용도와 사용법에 대한 설명입니다.
    
    ### `cursor`의 목적
    
    1. **SQL 쿼리 실행**:
        - `cursor`는 SQL 명령어를 실행하는 데 사용됩니다. 예를 들어, `SELECT`, `INSERT`, `UPDATE`, `DELETE` 명령어를 실행할 수 있습니다.
        - 예를 들어, 당신의 코드에서:
        여기서 `conn.cursor()`는 새로운 `cursor` 객체를 생성하고, `m_cursor.execute(m_query)`는 `m_query`에 저장된 SQL 쿼리를 실행합니다.
            
            ```python
            m_cursor = conn.cursor()
            m_cursor.execute(m_query)
            
            ```
            
    2. **데이터 가져오기**:
        - 쿼리를 실행한 후, `cursor`를 사용하여 결과를 가져올 수 있습니다.
        - `fetchall()`, `fetchone()`, `fetchmany()`와 같은 메서드를 사용하여 쿼리 결과를 가져옵니다.
        - 당신의 코드에서:`fetchall()`은 쿼리의 모든 행을 가져옵니다.
            
            ```python
            results = m_cursor.fetchall()
            
            ```
            
    3. **결과 집합 관리**:
        - `cursor`를 사용하여 결과 집합을 한 행씩 탐색할 수 있습니다.
        - 이를 통해 결과를 프로그래밍적으로 처리할 수 있습니다.
    4. **리소스 관리**:
        - `cursor`는 사용 후 닫아야 데이터베이스 리소스를 해제할 수 있습니다.
        - 이는 다음과 같이 수행됩니다:
            
            ```python
            m_cursor.close()
            
            ```
            
        - `cursor`를 닫으면 데이터베이스 서버가 할당한 메모리와 리소스가 해제됩니다.
    
    ### 코드에서의 `cursor` 사용 예
    
    여기서 `cursor`가 어떻게 사용되는지 살펴보겠습니다:
    
    1. **커서 생성**:
        
        ```python
        m_cursor = conn.cursor()
        
        ```
        
        - 데이터베이스 연결 `conn`과 연결된 `cursor` 객체를 생성합니다.
    2. **쿼리 실행**:
        
        ```python
        m_cursor.execute(m_query)
        
        ```
        
        - `cursor`를 사용하여 SQL 쿼리 (`"SELECT * FROM Orders;"`)를 실행합니다.
    3. **결과 가져오기**:
        
        ```python
        results = m_cursor.fetchall()
        
        ```
        
        - 쿼리 결과의 모든 행을 가져와 `results`에 저장합니다.
    4. **커서 닫기**:
        
        ```python
        m_cursor.close()
        
        ```
        
        - 사용이 끝난 후 `cursor`를 닫아 리소스를 해제합니다.
    
    ### 요약
    
    파이썬에서 데이터베이스와 상호작용할 때 `cursor`는 SQL 쿼리를 실행하고 데이터를 가져오는 데 필수적인 객체입니다. `cursor`를 사용하여 쿼리를 실행하고, 결과를 가져오며, 리소스를 관리할 수 있습니다.
    
- `extract_mysql_incremental.py`
    
    ```python
    import pymysql
    import csv
    import boto3
    import configparser
    import psycopg2
    
    # get db Redshift connection info
    parser = configparser.ConfigParser()
    parser.read("pipeline.conf")
    dbname = parser.get("aws_creds", "database")
    user = parser.get("aws_creds", "username")
    password = parser.get("aws_creds", "password")
    host = parser.get("aws_creds", "host")
    port = parser.get("aws_creds", "port")
    
    # connect to the redshift cluster
    rs_conn = psycopg2.connect(
        "dbname=" + dbname
        + " user=" + user
        + " password=" + password
        + " host=" + host
        + " port=" + port)
    
    rs_sql = """SELECT COALESCE(MAX(LastUpdated), '1900-01-01')
        FROM Orders;"""
    rs_cursor = rs_conn.cursor()
    rs_cursor.execute(rs_sql)
    result = rs_cursor.fetchone()
    
    # there's only one row and column returned
    last_updated_warehouse = result[0]
    
    rs_cursor.close()
    rs_conn.commit()
    
    # get the MySQL connection info and connect
    parser = configparser.ConfigParser()
    parser.read("pipeline.conf")
    hostname = parser.get("mysql_config", "hostname")
    port = parser.get("mysql_config", "port")
    username = parser.get("mysql_config", "username")
    dbname = parser.get("mysql_config", "database")
    password = parser.get("mysql_config", "password")
    
    conn = pymysql.connect(host=hostname,
            user=username,
            password=password,
            db=dbname,
            port=int(port))
    
    if conn is None:
      print("Error connecting to the MySQL database")
    else:
      print("MySQL connection established!")
    
    m_query = """SELECT *
        FROM Orders
        WHERE LastUpdated > %s;"""
    local_filename = "order_extract.csv"
    
    m_cursor = conn.cursor()
    m_cursor.execute(m_query, (last_updated_warehouse,))
    results = m_cursor.fetchall()
    
    with open(local_filename, 'w') as fp:
      csv_w = csv.writer(fp, delimiter='|')
      csv_w.writerows(results)
    
    fp.close()
    m_cursor.close()
    conn.close()
    
    # load the aws_boto_credentials values
    parser = configparser.ConfigParser()
    parser.read("pipeline.conf")
    access_key = parser.get(
        "aws_boto_credentials",
        "access_key")
    secret_key = parser.get(
        "aws_boto_credentials",
        "secret_key")
    bucket_name = parser.get(
        "aws_boto_credentials",
        "bucket_name")
    
    s3 = boto3.client(
        's3',
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key)
    
    s3_file = local_filename
    
    s3.upload_file(
        local_filename,
        bucket_name,
        s3_file)
    ```
    
- 이 코드는 Amazon Redshift와 MySQL 데이터베이스를 사용하여 데이터를 추출하고, 추출된 데이터를 CSV 파일로 저장한 후, 해당 파일을 AWS S3 버킷에 업로드하는 전체 프로세스를 자동화합니다.
    
    ### 1. Redshift 데이터베이스 연결 정보 로딩
    
    ```python
    import configparser
    import psycopg2
    
    # get db Redshift connection info
    parser = configparser.ConfigParser()
    parser.read("pipeline.conf")
    dbname = parser.get("aws_creds", "database")
    user = parser.get("aws_creds", "username")
    password = parser.get("aws_creds", "password")
    host = parser.get("aws_creds", "host")
    port = parser.get("aws_creds", "port")
    
    ```
    
    - `configparser`를 사용하여 `pipeline.conf` 파일에서 Amazon Redshift 데이터베이스 연결 정보를 읽어옵니다.
    - 데이터베이스 이름, 사용자 이름, 비밀번호, 호스트, 포트를 변수에 저장합니다.
    
    ### 2. Redshift 클러스터에 연결 및 최신 데이터 추출
    
    ```python
    # connect to the redshift cluster
    rs_conn = psycopg2.connect(
        "dbname=" + dbname
        + " user=" + user
        + " password=" + password
        + " host=" + host
        + " port=" + port)
    
    rs_sql = """SELECT COALESCE(MAX(LastUpdated), '1900-01-01')
        FROM Orders;"""
    rs_cursor = rs_conn.cursor()
    rs_cursor.execute(rs_sql)
    result = rs_cursor.fetchone()
    
    # there's only one row and column returned
    last_updated_warehouse = result[0]
    
    rs_cursor.close()
    rs_conn.commit()
    
    ```
    
    - `psycopg2` 라이브러리를 사용하여 Redshift 클러스터에 연결합니다.
    - `rs_sql` 쿼리를 실행하여 `Orders` 테이블의 `LastUpdated` 열에서 최대 날짜를 가져옵니다. 이 값을 통해 데이터의 최신 업데이트 시점을 알 수 있습니다.
    - `fetchone()` 메서드를 사용하여 쿼리 결과를 하나의 행으로 가져오고, `last_updated_warehouse` 변수에 저장합니다.
    - 쿼리 실행 후 커서를 닫고, 트랜잭션을 커밋합니다.
    
    ### 3. MySQL 데이터베이스 연결 정보 로딩 및 연결
    
    ```python
    import pymysql
    
    # get the MySQL connection info and connect
    parser = configparser.ConfigParser()
    parser.read("pipeline.conf")
    hostname = parser.get("mysql_config", "hostname")
    port = parser.get("mysql_config", "port")
    username = parser.get("mysql_config", "username")
    dbname = parser.get("mysql_config", "database")
    password = parser.get("mysql_config", "password")
    
    conn = pymysql.connect(host=hostname,
            user=username,
            password=password,
            db=dbname,
            port=int(port))
    
    if conn is None:
      print("Error connecting to the MySQL database")
    else:
      print("MySQL connection established!")
    
    ```
    
    - `configparser`를 사용하여 `pipeline.conf` 파일에서 MySQL 데이터베이스 연결 정보를 읽어옵니다.
    - `pymysql` 라이브러리를 사용하여 MySQL 데이터베이스에 연결합니다.
    - 연결이 성공하면 "MySQL connection established!"라는 메시지를 출력합니다.
    
    ### 4. MySQL 데이터 쿼리 및 CSV 파일로 저장
    
    ```python
    import csv
    
    m_query = """SELECT *
        FROM Orders
        WHERE LastUpdated > %s;"""
    local_filename = "order_extract.csv"
    
    m_cursor = conn.cursor()
    m_cursor.execute(m_query, (last_updated_warehouse,))
    results = m_cursor.fetchall()
    
    with open(local_filename, 'w') as fp:
      csv_w = csv.writer(fp, delimiter='|')
      csv_w.writerows(results)
    
    fp.close()
    m_cursor.close()
    conn.close()
    
    ```
    
    - `m_query` 쿼리를 사용하여 `Orders` 테이블에서 `LastUpdated`가 `last_updated_warehouse`보다 큰 데이터를 선택합니다.
    - 쿼리 결과를 `order_extract.csv` 파일로 저장합니다. 이 파일은 `|`를 구분자로 사용하는 CSV 파일입니다.
    - 작업이 끝난 후, 파일을 닫고, MySQL 커서와 연결을 닫습니다.
    
    ### 5. AWS S3에 CSV 파일 업로드
    
    ```python
    import boto3
    
    # load the aws_boto_credentials values
    parser = configparser.ConfigParser()
    parser.read("pipeline.conf")
    access_key = parser.get("aws_boto_credentials", "access_key")
    secret_key = parser.get("aws_boto_credentials", "secret_key")
    bucket_name = parser.get("aws_boto_credentials", "bucket_name")
    
    s3 = boto3.client(
        's3',
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key)
    
    s3_file = local_filename
    
    s3.upload_file(
        local_filename,
        bucket_name,
        s3_file)
    
    ```
    
    - `configparser`를 사용하여 `pipeline.conf` 파일에서 AWS S3에 접근하기 위한 인증 정보와 버킷 이름을 읽어옵니다.
    - `boto3` 라이브러리를 사용하여 AWS S3 클라이언트를 생성합니다.
    - `s3.upload_file()` 메서드를 사용하여 `order_extract.csv` 파일을 지정된 S3 버킷에 업로드합니다.
    
    ### 요약
    
    1. **Redshift 데이터베이스에서 최신 데이터 추출**: Redshift에서 `Orders` 테이블의 `LastUpdated` 열의 최신 날짜를 가져옵니다.
    2. **MySQL에서 데이터 추출**: MySQL에서 `LastUpdated`가 Redshift에서 가져온 최신 날짜 이후의 데이터를 쿼리합니다.
    3. **CSV 파일로 저장**: 쿼리 결과를 CSV 파일로 저장합니다.
    4. **S3에 업로드**: 저장된 CSV 파일을 AWS S3 버킷에 업로드합니다.
    
    이 코드는 데이터 파이프라인을 구성하여 Redshift와 MySQL 간의 데이터를 동기화하고, 최종적으로 클라우드 스토리지에 백업하는 과정을 자동화합니다.
    
- Amazon Redshift
    
    Amazon Redshift는 AWS의 완전 관리형 데이터 웨어하우스 서비스로, 대량의 데이터를 분석하고 비즈니스 인사이트를 얻기 위해 사용됩니다. Redshift의 주요 용도와 이점은 다음과 같습니다:
    
    ### Redshift의 주요 용도
    
    1. **대규모 데이터 분석**:
        - Redshift는 대규모 데이터 세트를 저장하고 분석할 수 있는 기능을 제공합니다. 이는 데이터 웨어하우스에서 SQL 쿼리를 실행하여 데이터를 분석하는 데 유용합니다.
    2. **빅데이터 처리**:
        - Redshift는 분산 아키텍처를 사용하여 데이터를 여러 노드에 분산시켜 저장하고 처리합니다. 이를 통해 대량의 데이터를 빠르게 쿼리하고 분석할 수 있습니다.
    3. **데이터 집계 및 보고**:
        - 데이터 집계, 분석 및 비즈니스 인텔리전스(BI) 도구와 통합하여 보고서를 생성할 수 있습니다. Redshift는 OLAP(온라인 분석 처리) 쿼리에 최적화되어 있습니다.
    4. **ETL(Extract, Transform, Load) 과정**:
        - 데이터 웨어하우스에서 ETL 작업을 수행할 때, Redshift는 데이터를 추출하고 변환한 후 로드하여 분석할 수 있는 중앙 저장소 역할을 합니다.
    5. **데이터 통합**:
        - 다양한 소스에서 데이터를 가져와 통합하여 분석할 수 있습니다. Redshift는 AWS의 다른 서비스와 쉽게 통합되며, 데이터 파이프라인을 통해 다양한 데이터 소스를 Redshift로 이동시킬 수 있습니다.
    
    ### Redshift의 이점
    
    1. **확장성**:
        - Redshift는 클러스터의 노드를 추가하여 저장 용량과 처리 성능을 확장할 수 있습니다. 필요에 따라 자동으로 스케일링할 수 있습니다.
    2. **비용 효율성**:
        - 사용량 기반의 요금제로, 필요한 만큼만 지불합니다. 온디맨드 요금제와 예약 인스턴스를 선택할 수 있어 비용을 절감할 수 있습니다.
    3. **성능 최적화**:
        - Redshift는 컬럼 저장소, 데이터 압축, 쿼리 최적화 등을 통해 높은 성능을 제공합니다. 데이터 쿼리 성능을 극대화하는 다양한 기능이 내장되어 있습니다.
    4. **AWS와의 통합**:
        - AWS의 다른 서비스와 원활하게 통합됩니다. 예를 들어, AWS S3에서 데이터를 로드하거나, AWS Glue를 사용하여 ETL 작업을 자동화할 수 있습니다.
    5. **보안**:
        - 데이터 암호화, 네트워크 보안, IAM(Identity and Access Management) 등의 기능을 통해 데이터 보안을 강화할 수 있습니다.
    
    ### 코드에서의 Redshift 사용
    
    코드에서는 Redshift를 다음과 같이 활용하고 있습니다:
    
    1. **최신 데이터의 마지막 업데이트 시점 조회**:
        
        ```python
        rs_sql = """SELECT COALESCE(MAX(LastUpdated), '1900-01-01') FROM Orders;"""
        
        ```
        
        - Redshift에서 `Orders` 테이블의 `LastUpdated` 열의 최대 날짜를 조회하여, 데이터가 마지막으로 업데이트된 시점을 파악합니다. 이는 이후 MySQL에서 데이터를 추출할 때, 이미 가져온 데이터와 중복되지 않도록 하기 위한 기준이 됩니다.
    2. **Redshift와 MySQL 간 데이터 동기화**:
        - Redshift에서 최신 업데이트 시점을 확인한 후, MySQL에서 이 시점 이후의 데이터만을 추출하여 CSV 파일로 저장하고, 이를 AWS S3에 업로드합니다. 이 과정은 Redshift와 MySQL 간의 데이터 동기화 작업을 수행하는 데 사용됩니다.
    
    Redshift는 대량의 데이터 분석과 저장에 적합하며, 데이터 파이프라인에서 데이터 웨어하우스 역할을 하여 데이터 분석 및 보고를 위한 중앙 저장소를 제공합니다.